{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries + create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INCOME\t1\tUnder $22\t0 #11000\n",
    "#INCOME\t2\t$22\t001 - $40 #31000\n",
    "#INCOME\t3\t$40\t001 - $60 #50000\n",
    "#INCOME\t4\t$60\t001 - $80 #70000\n",
    "#INCOME\t5\t$80\t001 - $100 #90000\n",
    "#INCOME\t6\t$100\t000 - $129 #115000\n",
    "#INCOME\t7\t$130\t000 - $159 #145000\n",
    "#INCOME\t8\t$160\t000 + #180000\n",
    "\n",
    "def income_est(b):\n",
    "    if b == 1:\n",
    "        return 11000\n",
    "    \n",
    "    elif b == 2:\n",
    "        return 31000\n",
    "    \n",
    "    elif b == 3:\n",
    "        return 50000\n",
    "    \n",
    "    elif b == 4:\n",
    "        return 70000\n",
    "    \n",
    "    elif b == 5:\n",
    "        return 90000\n",
    "    \n",
    "    elif b == 6:\n",
    "        return 115000\n",
    "    \n",
    "    elif b == 7:\n",
    "        return 145000\n",
    "    \n",
    "    elif b == 8:\n",
    "        return 180000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGE\t1\t0-4\n",
    "#AGE\t2\t5-12\n",
    "#AGE\t3\t13-17\n",
    "#AGE\t4\t18-24\n",
    "#AGE\t5\t25-39\n",
    "#AGE\t6\t40-54\n",
    "#AGE\t7\t55-69\n",
    "#AGE\t8\t70\n",
    "\n",
    "def age(x):\n",
    "    if x == 1:\n",
    "        return 21\n",
    "    \n",
    "    elif x == 2:\n",
    "        return 21\n",
    "    \n",
    "    elif x == 3:\n",
    "        return 21\n",
    "    \n",
    "    elif x == 4:\n",
    "        return 21\n",
    "    \n",
    "    elif x == 5:\n",
    "        return 32\n",
    "    \n",
    "    elif x == 6:\n",
    "        return 47\n",
    "    \n",
    "    elif x == 7:\n",
    "        return 62\n",
    "    \n",
    "    elif x == 8:\n",
    "        return 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIFESTAGE\t1\tYOUNG SUNGLES & COUPLES\n",
    "#LIFESTAGE\t2\tYOUNG FAMILIES\n",
    "#LIFESTAGE\t3\tMIXED FAMILIES\n",
    "#LIFESTAGE\t4\tOLDER FAMILIES\n",
    "#LIFESTAGE\t5\tOLDER SINGLES & COUPLES\n",
    "#LIFESTAGE\t6\tADULT HOUSEHOLDS\n",
    "#LIFESTAGE\t7\tUNDEFINED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import projection factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection factors\n",
    "df19pf = pd.read_csv('2019/TGI_FACTOR_MAT_20191228.CSV', header=None)\n",
    "df19pf.columns = ['HHID', 'REGION', 'PF']\n",
    "\n",
    "df18pf = pd.read_csv('2018/TGI_FACTOR_MAT_20181229.CSV', header=None)\n",
    "df18pf.columns = ['HHID', 'REGION', 'PF']\n",
    "\n",
    "df17pf = pd.read_csv('2017/TGI_FACTOR_MAT_20171230.CSV', header=None)\n",
    "df17pf.columns = ['HHID', 'REGION', 'PF']\n",
    "\n",
    "df16pf = pd.read_csv('2016/TGI_FACTOR_MAT_20161231.CSV', header=None)\n",
    "df16pf.columns = ['HHID', 'REGION', 'PF']\n",
    "\n",
    "df15pf = pd.read_csv('2015/TGI_FACTOR_MAT_20151226.CSV', header=None)\n",
    "df15pf.columns = ['HHID', 'REGION', 'PF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection factors\n",
    "df19pf = pd.read_csv('2019/TGI_FACTOR_MAT_20191228.CSV', header=None)\n",
    "df19pf.columns = ['HHID', 'REGION', 'PF']\n",
    "df19pf['YEAR'] = 2019\n",
    "\n",
    "df18pf = pd.read_csv('2018/TGI_FACTOR_MAT_20181229.CSV', header=None)\n",
    "df18pf.columns = ['HHID', 'REGION', 'PF']\n",
    "df18pf['YEAR'] = 2018\n",
    "\n",
    "df17pf = pd.read_csv('2017/TGI_FACTOR_MAT_20171230.CSV', header=None)\n",
    "df17pf.columns = ['HHID', 'REGION', 'PF']\n",
    "df17pf['YEAR'] = 2017\n",
    "\n",
    "df16pf = pd.read_csv('2016/TGI_FACTOR_MAT_20161231.CSV', header=None)\n",
    "df16pf.columns = ['HHID', 'REGION', 'PF']\n",
    "df16pf['YEAR'] = 2016\n",
    "\n",
    "df15pf = pd.read_csv('2015/TGI_FACTOR_MAT_20151226.CSV', header=None)\n",
    "df15pf.columns = ['HHID', 'REGION', 'PF']\n",
    "df15pf['YEAR'] = 2015\n",
    "\n",
    "dfpf = pd.concat([df15pf, df16pf, df17pf, df18pf, df19pf])\n",
    "dfpf['YEAR-HHID'] = dfpf['YEAR'].astype(str) + '|' + dfpf['HHID'].astype(str)\n",
    "dfpf['REGION 2'] = dfpf['REGION'].replace(1, 'SYD').replace(2, 'MEL').replace(3, 'BRI').replace(4, 'ADE').replace(5, 'PER').replace(6, 'Other').replace(7, 'Other').replace(8, 'Other').replace(9, 'Other').replace(10, 'Other').replace(11, 'Other').replace(12, 'Other').replace(13, 'Other')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import household size information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# household size\n",
    "demog = pd.read_csv('TGI_DEMOG_BY_QTR.CSV')\n",
    "demog.columns = ['HHID', 'QTR', 'LIFESTAGE', 'HH SZ', 'INCOME', 'AFFLUENCE', 'ETHNICITY',\n",
    "              'MICROSEGMENT', 'HEAD EDUCATION', 'AGE1', 'AGE2', 'AGE3', 'AGE4', 'AGE5',\n",
    "              'AGE6', 'AGE7', 'AGE8', 'AGE9', 'AGE10', 'AGE11', 'AGE12', 'AGE13', 'AGE14', 'AGE15',\n",
    "              'AGE16', 'AGE17', 'AGE18', 'AGE19', 'AGE20', 'SEX1', 'SEX2', 'SEX3', 'SEX4', 'SEX5',\n",
    "               'SEX6', 'SEX7', 'SEX8', 'SEX9', 'SEX10','SEX11', 'SEX12', 'SEX13', 'SEX14', 'SEX15',\n",
    "              'SEX16', 'SEX17', 'SEX18', 'SEX19', 'SEX20']\n",
    "\n",
    "\n",
    "# BASIC DEMOGRAPHIC VARIABLES\n",
    "demog['INCOME_VAL'] = demog['INCOME'].apply(income_est)\n",
    "\n",
    "for n in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]:\n",
    "    c1 = ~demog['AGE' + str(n)].isnull()\n",
    "    c2 = demog['SEX' + str(n)].isnull()\n",
    "    demog['SEX' + str(n)] = np.where(c1&c2, 1, demog['SEX' + str(n)])\n",
    "    \n",
    "demog = demog[demog['HH SZ'] != 0]    \n",
    "\n",
    "\n",
    "# MORE DEMOGRPAHIC VARIABLES\n",
    "for n in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]:\n",
    "    demog['P' + str(n)] = demog['SEX' + str(n)].fillna('0').replace(1, 'M').replace(2, 'F') + demog['AGE' + str(n)].fillna('0').astype(int).astype(str)\n",
    "\n",
    "adult_men = {'M3', 'M4', 'M5', 'M6', 'M7', 'M8'}\n",
    "adult_women = {'F3', 'F4', 'F5', 'F6', 'F7', 'F8'}\n",
    "children = {'M1', 'M2', 'F1', 'F2'}\n",
    "\n",
    "demog['Children'] = demog[['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9',\n",
    "       'P10', 'P11', 'P12', 'P13', 'P14', 'P15']].apply(lambda row: sum(row.isin(children)), axis=1)\n",
    "\n",
    "demog['Adolescent and adult men'] = demog[['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9',\n",
    "       'P10', 'P11', 'P12', 'P13', 'P14', 'P15']].apply(lambda row: sum(row.isin(adult_men)), axis=1)\n",
    "\n",
    "demog['Adolescent and adult women'] = demog[['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9',\n",
    "       'P10', 'P11', 'P12', 'P13', 'P14', 'P15']].apply(lambda row: sum(row.isin(adult_women)), axis=1)\n",
    "\n",
    "\n",
    "# SUBSET\n",
    "demog19 = demog[demog['QTR'] == '2019-1']\n",
    "demog19 = demog19[['HHID', 'HH SZ', 'INCOME_VAL', 'Children', 'Adolescent and adult men', 'Adolescent and adult women']]\n",
    "demog19 = demog19[demog19['HHID'].isin(df19pf['HHID'])]\n",
    "\n",
    "demog18 = demog[demog['QTR'] == '2018-1']\n",
    "demog18 = demog18[['HHID', 'HH SZ', 'INCOME_VAL', 'Children', 'Adolescent and adult men', 'Adolescent and adult women']]\n",
    "demog18 = demog18[demog18['HHID'].isin(df18pf['HHID'])]\n",
    "\n",
    "demog17 = demog[demog['QTR'] == '2017-1']\n",
    "demog17 = demog17[['HHID', 'HH SZ', 'INCOME_VAL', 'Children', 'Adolescent and adult men', 'Adolescent and adult women']]\n",
    "demog17 = demog17[demog17['HHID'].isin(df17pf['HHID'])]\n",
    "\n",
    "demog16 = demog[demog['QTR'] == '2016-1']\n",
    "demog16 = demog16[['HHID', 'HH SZ', 'INCOME_VAL',  'Children', 'Adolescent and adult men', 'Adolescent and adult women']]\n",
    "demog16 = demog16[demog16['HHID'].isin(df16pf['HHID'])]\n",
    "\n",
    "demog15 = demog[demog['QTR'] == '2015-1']\n",
    "demog15 = demog15[['HHID', 'HH SZ', 'INCOME_VAL',  'Children', 'Adolescent and adult men', 'Adolescent and adult women']]\n",
    "demog15 = demog15[demog15['HHID'].isin(df15pf['HHID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "demog = demog[demog['QTR'].str.split('-').str[-1] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Import SEIFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seifa\n",
    "seifa = pd.read_excel('SEIFA deciles TGI 2015 onwards.xlsx')\n",
    "\n",
    "# CHANGES MADE HERE IN MAY 2024\n",
    "seifa['SES'] = 'Q1'\n",
    "seifa['SES'] = np.where(seifa['irsdscoreI']>=3, 'Q2', seifa['SES'])\n",
    "seifa['SES'] = np.where(seifa['irsdscoreI']>=5, 'Q3', seifa['SES'])\n",
    "seifa['SES'] = np.where(seifa['irsdscoreI']>=7, 'Q4', seifa['SES'])\n",
    "seifa['SES'] = np.where(seifa['irsdscoreI']>=9, 'Q5', seifa['SES'])\n",
    "\n",
    "seifa19 = seifa[seifa['Qtr'] == '2019-1']\n",
    "seifa19 = seifa19[['Mapped HHID', 'irsdscoreI', 'SES']]\n",
    "seifa19 = seifa19[seifa19['Mapped HHID'].isin(df19pf['HHID'])]\n",
    "seifa19.columns = ['HHID', 'irsdscoreI', 'SES']\n",
    "\n",
    "seifa18 = seifa[seifa['Qtr'] == '2018-1']\n",
    "seifa18 = seifa18[['Mapped HHID', 'irsdscoreI', 'SES']]\n",
    "seifa18 = seifa18[seifa18['Mapped HHID'].isin(df18pf['HHID'])]\n",
    "seifa18.columns = ['HHID', 'irsdscoreI', 'SES']\n",
    "\n",
    "seifa17 = seifa[seifa['Qtr'] == '2017-1']\n",
    "seifa17 = seifa17[['Mapped HHID', 'irsdscoreI', 'SES']]\n",
    "seifa17 = seifa17[seifa17['Mapped HHID'].isin(df17pf['HHID'])]\n",
    "seifa17.columns = ['HHID', 'irsdscoreI', 'SES']\n",
    "\n",
    "seifa16 = seifa[seifa['Qtr'] == '2016-1']\n",
    "seifa16 = seifa16[['Mapped HHID', 'irsdscoreI', 'SES']]\n",
    "seifa16 = seifa16[seifa16['Mapped HHID'].isin(df16pf['HHID'])]\n",
    "seifa16.columns = ['HHID', 'irsdscoreI', 'SES']\n",
    "\n",
    "seifa15 = seifa[seifa['Qtr'] == '2015-1']\n",
    "seifa15 = seifa15[['Mapped HHID', 'irsdscoreI', 'SES']]\n",
    "seifa15 = seifa15[seifa15['Mapped HHID'].isin(df15pf['HHID'])]\n",
    "seifa15.columns = ['HHID', 'irsdscoreI', 'SES']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Import product dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl = pd.read_csv('barc_lookup_full_v6.csv', dtype = {'BARCODE':object})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Import sales datasets and aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "majors = ['Vegetables',\n",
    " 'Fruit',\n",
    " 'Grains',\n",
    " 'Meat and alternatives',\n",
    " 'Dairy products and alternatives',\n",
    " 'Non-alcoholic beverages',\n",
    " 'Confectionery and snacks',\n",
    " 'Biscuits, cakes, and desserts',\n",
    " 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "minors = ['Processed meat',\n",
    " 'Vegetables',\n",
    " 'Ready meals',\n",
    " 'Non-sugar sweetened beverages',\n",
    " 'Biscuits',\n",
    " 'Fats and oils',\n",
    " 'Pasta, rice, and other cereals',\n",
    " 'Sauces, dressings, spreads, and dips',\n",
    " 'Milk',\n",
    " 'Fruit',\n",
    " 'Sugar-based confectionery',\n",
    " 'Desserts',\n",
    " 'Breakfast cereals',\n",
    " 'Cakes, muffins, and pastries',\n",
    " 'Cheese and cream',\n",
    " 'Sugar-sweetened beverages',\n",
    " 'Nuts and seeds',\n",
    " 'Tea and coffee',\n",
    " 'Poultry',\n",
    " 'Eggs',\n",
    " 'Yoghurt',\n",
    " 'Chocolate-based confectionery',\n",
    " 'Sugars, honey, and related products',\n",
    " 'Fish and seafoods',\n",
    " 'Legumes/beans',\n",
    " 'Snackfoods',\n",
    " 'Bread',\n",
    " 'Ice-cream',\n",
    " 'Red meat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16877914.6\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "df19 = pd.read_csv('2019/TGI_DATA_201901_201913.CSV', header = None, dtype = {0:object})\n",
    "\n",
    "# rename columns\n",
    "df19.columns = ['BARCODE', 'GENERATION', 'HHID', 'SHOP', 'TRANS DATE', 'TRANS TIME',\n",
    "               'QTY', 'PRICE', 'EP', 'ON PROMO', 'PLU', 'WGT']\n",
    "\n",
    "# exclude households with inconsistent records\n",
    "df19 = df19[df19['HHID'].isin(df19pf['HHID'])]\n",
    "\n",
    "# exclude non-foods by inner joining with food barcode lookup dataset\n",
    "df19 = df19.merge(bl[['BARCODE', 'SIZE', 'UOM', 'FS Major category', 'cat']], on = 'BARCODE', how = 'inner')\n",
    "\n",
    "# exclude purchases with PRICE of $0\n",
    "df19 = df19[df19['PRICE'] != 0]\n",
    "\n",
    "# adjust for extreme purchases\n",
    "df19['QTY 2'] = df19['QTY']*df19['EP']\n",
    "\n",
    "# determine the quarter of each transaction\n",
    "df19['MONTH'] = df19['TRANS DATE'].str.split('/').str[1]\n",
    "df19['YEAR'] = df19['TRANS DATE'].str.split('/').str[2]\n",
    "df19['QTR'] = np.NaN\n",
    "df19['QTR'] = np.where(df19['MONTH'].isin(['01', '02', '03']), 1, df19['QTR'])\n",
    "df19['QTR'] = np.where(df19['MONTH'].isin(['04', '05', '06']), 2, df19['QTR'])\n",
    "df19['QTR'] = np.where(df19['MONTH'].isin(['07', '08', '09']), 3, df19['QTR'])\n",
    "df19['QTR'] = np.where(df19['MONTH'].isin(['10', '11', '12']), 4, df19['QTR'])\n",
    "df19['QTR'] = np.where(df19['YEAR'] == '2018', 1, df19['QTR'])\n",
    "\n",
    "# adjust for inflation using quarterly CPI\n",
    "df19['PRICE 2'] = np.NaN\n",
    "df19['PRICE 2'] = np.where(df19['QTR'] == 1, df19['PRICE'], df19['PRICE 2'])\n",
    "df19['PRICE 2'] = np.where(df19['QTR'] == 2, df19['PRICE']*(109/108.6), df19['PRICE 2'])\n",
    "df19['PRICE 2'] = np.where(df19['QTR'] == 3, df19['PRICE']*(109/109), df19['PRICE 2'])\n",
    "df19['PRICE 2'] = np.where(df19['QTR'] == 4, df19['PRICE']*(109/110.4), df19['PRICE 2'])\n",
    "\n",
    "# create clean size column\n",
    "df19['SIZE 2'] = np.where(df19['UOM'].isin(['GM', 'ML']), df19['SIZE'], np.NaN)\n",
    "\n",
    "# create columns for expenditure and purchase volume\n",
    "df19['EXP'] = df19['PRICE 2']*df19['QTY 2']\n",
    "df19['PUR VOL'] = df19['SIZE 2']*df19['QTY 2']\n",
    "\n",
    "print(df19['QTY 2'].sum())\n",
    "\n",
    "# isolate required columns\n",
    "df19 = df19[['HHID', 'FS Major category', 'cat', 'EXP', 'PUR VOL', 'ON PROMO', 'QTY 2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 53s\n",
      "Parser   : 2.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "HHIDs = df19['HHID'].unique().tolist()\n",
    "for HHID in HHIDs:\n",
    "    subset = df19[df19['HHID'] == HHID]\n",
    "    row = {}\n",
    "    row[' HHID'] = HHID\n",
    "    row[' YEAR'] = '2019'\n",
    "    tot_exp = subset['EXP'].sum()\n",
    "    row['TOT EXP'] = tot_exp\n",
    "    for major in majors:\n",
    "        subset2 = subset[subset['FS Major category'] == major]\n",
    "        row[major + ' SHARE'] = subset2['EXP'].sum()/tot_exp\n",
    "        subset3 = subset2[~subset2['PUR VOL'].isnull()]\n",
    "        row[major + ' PRICE'] = (1000)*(subset3['EXP'].sum()/subset3['PUR VOL'].sum())\n",
    "        row[major + ' ON PROMO'] = (subset2['EXP']*subset2['ON PROMO']).sum()/subset2['EXP'].sum()\n",
    "        \n",
    "    data.append(row)\n",
    "    \n",
    "df19_agg = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df19_agg.to_excel('df19_agg_major.xlsx', index=False)\n",
    "del df19_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "HHIDs = df19['HHID'].unique().tolist()\n",
    "for HHID in HHIDs:\n",
    "    subset = df19[df19['HHID'] == HHID]\n",
    "    row = {}\n",
    "    row[' HHID'] = HHID\n",
    "    row[' YEAR'] = '2019'\n",
    "    tot_exp = subset['EXP'].sum()\n",
    "    row['TOT EXP'] = tot_exp\n",
    "    for minor in minors:\n",
    "        subset2 = subset[subset['cat'] == minor]\n",
    "        row[minor + ' SHARE'] = subset2['EXP'].sum()/tot_exp\n",
    "        subset3 = subset2[~subset2['PUR VOL'].isnull()]\n",
    "        row[minor + ' PRICE'] = (1000)*(subset3['EXP'].sum()/subset3['PUR VOL'].sum())\n",
    "        row[minor + ' ON PROMO'] = (subset2['EXP']*subset2['ON PROMO']).sum()/subset2['EXP'].sum()\n",
    "        \n",
    "    data.append(row)\n",
    "    \n",
    "df19_agg_minor = pd.DataFrame(data)\n",
    "\n",
    "df19_agg_minor.to_excel('df19_agg_minor.xlsx', index=False)\n",
    "del df19_agg_minor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16861012.699999977\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "df18 = pd.read_csv('2018/TGI_DATA_201801_201813.CSV', header = None, dtype = {0:object})\n",
    "\n",
    "# rename columns\n",
    "df18.columns = ['BARCODE', 'GENERATION', 'HHID', 'SHOP', 'TRANS DATE', 'TRANS TIME',\n",
    "               'QTY', 'PRICE', 'EP', 'ON PROMO', 'PLU', 'WGT']\n",
    "\n",
    "# exclude households with inconsistent records\n",
    "df18 = df18[df18['HHID'].isin(df18pf['HHID'])]\n",
    "\n",
    "# exclude non-foods by inner joining with food barcode lookup dataset\n",
    "df18 = df18.merge(bl[['BARCODE', 'SIZE', 'UOM', 'FS Major category', 'cat']], on = 'BARCODE', how = 'inner')\n",
    "\n",
    "# exclude purchases with PRICE of $0\n",
    "df18 = df18[df18['PRICE'] != 0]\n",
    "\n",
    "# adjust for extreme purchases\n",
    "df18['QTY 2'] = df18['QTY']*df18['EP']\n",
    "\n",
    "# determine the quarter of each transaction\n",
    "df18['MONTH'] = df18['TRANS DATE'].str.split('/').str[1]\n",
    "df18['YEAR'] = df18['TRANS DATE'].str.split('/').str[2]\n",
    "df18['QTR'] = np.NaN\n",
    "df18['QTR'] = np.where(df18['MONTH'].isin(['01', '02', '03']), 1, df18['QTR'])\n",
    "df18['QTR'] = np.where(df18['MONTH'].isin(['04', '05', '06']), 2, df18['QTR'])\n",
    "df18['QTR'] = np.where(df18['MONTH'].isin(['07', '08', '09']), 3, df18['QTR'])\n",
    "df18['QTR'] = np.where(df18['MONTH'].isin(['10', '11', '12']), 4, df18['QTR'])\n",
    "df18['QTR'] = np.where(df18['YEAR'] == '2017', 1, df18['QTR'])\n",
    "\n",
    "# adjust for inflation using quarterly CPI\n",
    "df18['PRICE 2'] = np.NaN\n",
    "df18['PRICE 2'] = np.where(df18['QTR'] == 1, df18['PRICE']*(109/106.5), df18['PRICE 2'])\n",
    "df18['PRICE 2'] = np.where(df18['QTR'] == 2, df18['PRICE']*(109/106.1), df18['PRICE 2'])\n",
    "df18['PRICE 2'] = np.where(df18['QTR'] == 3, df18['PRICE']*(109/106.6), df18['PRICE 2'])\n",
    "df18['PRICE 2'] = np.where(df18['QTR'] == 4, df18['PRICE']*(109/107.6), df18['PRICE 2'])\n",
    "\n",
    "# create clean size column\n",
    "df18['SIZE 2'] = np.where(df18['UOM'].isin(['GM', 'ML']), df18['SIZE'], np.NaN)\n",
    "\n",
    "# create columns for expenditure and purchase volume\n",
    "df18['EXP'] = df18['PRICE 2']*df18['QTY 2']\n",
    "df18['PUR VOL'] = df18['SIZE 2']*df18['QTY 2']\n",
    "\n",
    "print(df18['QTY 2'].sum())\n",
    "\n",
    "# isolate required columns\n",
    "df18 = df18[['HHID', 'FS Major category', 'cat', 'EXP', 'PUR VOL', 'ON PROMO', 'QTY 2']]\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "HHIDs = df18['HHID'].unique().tolist()\n",
    "for HHID in HHIDs:\n",
    "    subset = df18[df18['HHID'] == HHID]\n",
    "    row = {}\n",
    "    row[' HHID'] = HHID\n",
    "    row[' YEAR'] = '2018'\n",
    "    tot_exp = subset['EXP'].sum()\n",
    "    row['TOT EXP'] = tot_exp\n",
    "    for major in majors:\n",
    "        subset2 = subset[subset['FS Major category'] == major]\n",
    "        row[major + ' SHARE'] = subset2['EXP'].sum()/tot_exp\n",
    "        subset3 = subset2[~subset2['PUR VOL'].isnull()]\n",
    "        row[major + ' PRICE'] = (1000)*(subset3['EXP'].sum()/subset3['PUR VOL'].sum())\n",
    "        \n",
    "    data.append(row)\n",
    "    \n",
    "df18_agg = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df18_agg.to_excel('df18_agg.xlsx', index=False)\n",
    "del df18_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "HHIDs = df18['HHID'].unique().tolist()\n",
    "for HHID in HHIDs:\n",
    "    subset = df18[df18['HHID'] == HHID]\n",
    "    row = {}\n",
    "    row[' HHID'] = HHID\n",
    "    row[' YEAR'] = '2018'\n",
    "    tot_exp = subset['EXP'].sum()\n",
    "    row['TOT EXP'] = tot_exp\n",
    "    for minor in minors:\n",
    "        subset2 = subset[subset['cat'] == minor]\n",
    "        row[minor + ' SHARE'] = subset2['EXP'].sum()/tot_exp\n",
    "        subset3 = subset2[~subset2['PUR VOL'].isnull()]\n",
    "        row[minor + ' PRICE'] = (1000)*(subset3['EXP'].sum()/subset3['PUR VOL'].sum())\n",
    "        row[minor + ' ON PROMO'] = (subset2['EXP']*subset2['ON PROMO']).sum()/subset2['EXP'].sum()\n",
    "        \n",
    "    data.append(row)\n",
    "    \n",
    "df18_agg_minor = pd.DataFrame(data)\n",
    "\n",
    "df18_agg_minor.to_excel('df18_agg_minor.xlsx', index=False)\n",
    "del df18_agg_minor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17578031.399999972\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "df17 = pd.read_csv('2017/TGI_DATA_201701_201713.CSV', header = None, dtype = {0:object})\n",
    "\n",
    "# rename columns\n",
    "df17.columns = ['BARCODE', 'GENERATION', 'HHID', 'SHOP', 'TRANS DATE', 'TRANS TIME',\n",
    "               'QTY', 'PRICE', 'EP', 'ON PROMO', 'PLU', 'WGT']\n",
    "\n",
    "# exclude households with inconsistent records\n",
    "df17 = df17[df17['HHID'].isin(df17pf['HHID'])]\n",
    "\n",
    "# exclude non-foods by inner joining with food barcode lookup dataset\n",
    "df17 = df17.merge(bl[['BARCODE', 'SIZE', 'UOM', 'FS Major category', 'cat']], on = 'BARCODE', how = 'inner')\n",
    "\n",
    "# exclude purchases with PRICE of $0\n",
    "df17 = df17[df17['PRICE'] != 0]\n",
    "\n",
    "# adjust for extreme purchases\n",
    "df17['QTY 2'] = df17['QTY']*df17['EP']\n",
    "\n",
    "# determine the quarter of each transaction\n",
    "df17['MONTH'] = df17['TRANS DATE'].str.split('/').str[1]\n",
    "df17['YEAR'] = df17['TRANS DATE'].str.split('/').str[2]\n",
    "df17['QTR'] = np.NaN\n",
    "df17['QTR'] = np.where(df17['MONTH'].isin(['01', '02', '03']), 1, df17['QTR'])\n",
    "df17['QTR'] = np.where(df17['MONTH'].isin(['04', '05', '06']), 2, df17['QTR'])\n",
    "df17['QTR'] = np.where(df17['MONTH'].isin(['07', '08', '09']), 3, df17['QTR'])\n",
    "df17['QTR'] = np.where(df17['MONTH'].isin(['10', '11', '12']), 4, df17['QTR'])\n",
    "df17['QTR'] = np.where(df17['YEAR'] == '2016', 1, df17['QTR'])\n",
    "\n",
    "# adjust for inflation using quarterly CPI\n",
    "df17['PRICE 2'] = np.NaN\n",
    "df17['PRICE 2'] = np.where(df17['QTR'] == 1, df17['PRICE']*(109/106.0), df17['PRICE 2'])\n",
    "df17['PRICE 2'] = np.where(df17['QTR'] == 2, df17['PRICE']*(109/105.8), df17['PRICE 2'])\n",
    "df17['PRICE 2'] = np.where(df17['QTR'] == 3, df17['PRICE']*(109/104.9), df17['PRICE 2'])\n",
    "df17['PRICE 2'] = np.where(df17['QTR'] == 4, df17['PRICE']*(109/106.0), df17['PRICE 2'])\n",
    "\n",
    "# create clean size column\n",
    "df17['SIZE 2'] = np.where(df17['UOM'].isin(['GM', 'ML']), df17['SIZE'], np.NaN)\n",
    "\n",
    "# create columns for expenditure and purchase volume\n",
    "df17['EXP'] = df17['PRICE 2']*df17['QTY 2']\n",
    "df17['PUR VOL'] = df17['SIZE 2']*df17['QTY 2']\n",
    "\n",
    "print(df17['QTY 2'].sum())\n",
    "\n",
    "# isolate required columns\n",
    "df17 = df17[['HHID', 'FS Major category', 'cat', 'EXP', 'PUR VOL', 'ON PROMO', 'QTY 2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "HHIDs = df17['HHID'].unique().tolist()\n",
    "for HHID in HHIDs:\n",
    "    subset = df17[df17['HHID'] == HHID]\n",
    "    row = {}\n",
    "    row[' HHID'] = HHID\n",
    "    row[' YEAR'] = '2017'\n",
    "    tot_exp = subset['EXP'].sum()\n",
    "    row['TOT EXP'] = tot_exp\n",
    "    for major in majors:\n",
    "        subset2 = subset[subset['FS Major category'] == major]\n",
    "        row[major + ' SHARE'] = subset2['EXP'].sum()/tot_exp\n",
    "        subset3 = subset2[~subset2['PUR VOL'].isnull()]\n",
    "        row[major + ' PRICE'] = (1000)*(subset3['EXP'].sum()/subset3['PUR VOL'].sum())\n",
    "        \n",
    "    data.append(row)\n",
    "    \n",
    "df17_agg = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df17_agg.to_excel('df17_agg.xlsx', index=False)\n",
    "del df17_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "HHIDs = df17['HHID'].unique().tolist()\n",
    "for HHID in HHIDs:\n",
    "    subset = df17[df17['HHID'] == HHID]\n",
    "    row = {}\n",
    "    row[' HHID'] = HHID\n",
    "    row[' YEAR'] = '2017'\n",
    "    tot_exp = subset['EXP'].sum()\n",
    "    row['TOT EXP'] = tot_exp\n",
    "    for minor in minors:\n",
    "        subset2 = subset[subset['cat'] == minor]\n",
    "        row[minor + ' SHARE'] = subset2['EXP'].sum()/tot_exp\n",
    "        subset3 = subset2[~subset2['PUR VOL'].isnull()]\n",
    "        row[minor + ' PRICE'] = (1000)*(subset3['EXP'].sum()/subset3['PUR VOL'].sum())\n",
    "        row[minor + ' ON PROMO'] = (subset2['EXP']*subset2['ON PROMO']).sum()/subset2['EXP'].sum()\n",
    "        \n",
    "    data.append(row)\n",
    "    \n",
    "df17_agg_minor = pd.DataFrame(data)\n",
    "\n",
    "df17_agg_minor.to_excel('df17_agg_minor.xlsx', index=False)\n",
    "del df17_agg_minor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18028799.689999986\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "df16 = pd.read_csv('2016/TGI_DATA_201601_201613.CSV', header = None, dtype = {0:object})\n",
    "\n",
    "# rename columns\n",
    "df16.columns = ['BARCODE', 'GENERATION', 'HHID', 'SHOP', 'TRANS DATE', 'TRANS TIME',\n",
    "               'QTY', 'PRICE', 'EP', 'ON PROMO', 'PLU', 'WGT']\n",
    "\n",
    "# exclude households with inconsistent records\n",
    "df16 = df16[df16['HHID'].isin(df16pf['HHID'])]\n",
    "\n",
    "# exclude non-foods by inner joining with food barcode lookup dataset\n",
    "df16 = df16.merge(bl[['BARCODE', 'SIZE', 'UOM', 'FS Major category', 'cat']], on = 'BARCODE', how = 'inner')\n",
    "\n",
    "# exclude purchases with PRICE of $0\n",
    "df16 = df16[df16['PRICE'] != 0]\n",
    "\n",
    "# adjust for extreme purchases\n",
    "df16['QTY 2'] = df16['QTY']*df16['EP']\n",
    "\n",
    "# determine the quarter of each transaction\n",
    "df16['MONTH'] = df16['TRANS DATE'].str.split('/').str[1]\n",
    "df16['YEAR'] = df16['TRANS DATE'].str.split('/').str[2]\n",
    "df16['QTR'] = np.NaN\n",
    "df16['QTR'] = np.where(df16['MONTH'].isin(['01', '02', '03']), 1, df16['QTR'])\n",
    "df16['QTR'] = np.where(df16['MONTH'].isin(['04', '05', '06']), 2, df16['QTR'])\n",
    "df16['QTR'] = np.where(df16['MONTH'].isin(['07', '08', '09']), 3, df16['QTR'])\n",
    "df16['QTR'] = np.where(df16['MONTH'].isin(['10', '11', '12']), 4, df16['QTR'])\n",
    "df16['QTR'] = np.where(df16['YEAR'] == '2015', 1, df16['QTR'])\n",
    "\n",
    "# adjust for inflation using quarterly CPI\n",
    "df16['PRICE 2'] = np.NaN\n",
    "df16['PRICE 2'] = np.where(df16['QTR'] == 1, df16['PRICE']*(109/104.1), df16['PRICE 2'])\n",
    "df16['PRICE 2'] = np.where(df16['QTR'] == 2, df16['PRICE']*(109/103.8), df16['PRICE 2'])\n",
    "df16['PRICE 2'] = np.where(df16['QTR'] == 3, df16['PRICE']*(109/105.6), df16['PRICE 2'])\n",
    "df16['PRICE 2'] = np.where(df16['QTR'] == 4, df16['PRICE']*(109/106.2), df16['PRICE 2'])\n",
    "\n",
    "# create clean size column\n",
    "df16['SIZE 2'] = np.where(df16['UOM'].isin(['GM', 'ML']), df16['SIZE'], np.NaN)\n",
    "\n",
    "# create columns for expenditure and purchase volume\n",
    "df16['EXP'] = df16['PRICE 2']*df16['QTY 2']\n",
    "df16['PUR VOL'] = df16['SIZE 2']*df16['QTY 2']\n",
    "\n",
    "print(df16['QTY 2'].sum())\n",
    "\n",
    "# isolate required columns\n",
    "df16 = df16[['HHID', 'FS Major category', 'cat', 'EXP', 'PUR VOL', 'ON PROMO', 'QTY 2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "HHIDs = df16['HHID'].unique().tolist()\n",
    "for HHID in HHIDs:\n",
    "    subset = df16[df16['HHID'] == HHID]\n",
    "    row = {}\n",
    "    row[' HHID'] = HHID\n",
    "    row[' YEAR'] = '2016'\n",
    "    tot_exp = subset['EXP'].sum()\n",
    "    row['TOT EXP'] = tot_exp\n",
    "    for major in majors:\n",
    "        subset2 = subset[subset['FS Major category'] == major]\n",
    "        row[major + ' SHARE'] = subset2['EXP'].sum()/tot_exp\n",
    "        subset3 = subset2[~subset2['PUR VOL'].isnull()]\n",
    "        row[major + ' PRICE'] = (1000)*(subset3['EXP'].sum()/subset3['PUR VOL'].sum())\n",
    "        \n",
    "    data.append(row)\n",
    "    \n",
    "df16_agg = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df16_agg.to_excel('df16_agg.xlsx', index=False)\n",
    "del df16_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "HHIDs = df16['HHID'].unique().tolist()\n",
    "for HHID in HHIDs:\n",
    "    subset = df16[df16['HHID'] == HHID]\n",
    "    row = {}\n",
    "    row[' HHID'] = HHID\n",
    "    row[' YEAR'] = '2016'\n",
    "    tot_exp = subset['EXP'].sum()\n",
    "    row['TOT EXP'] = tot_exp\n",
    "    for minor in minors:\n",
    "        subset2 = subset[subset['cat'] == minor]\n",
    "        row[minor + ' SHARE'] = subset2['EXP'].sum()/tot_exp\n",
    "        subset3 = subset2[~subset2['PUR VOL'].isnull()]\n",
    "        row[minor + ' PRICE'] = (1000)*(subset3['EXP'].sum()/subset3['PUR VOL'].sum())\n",
    "        row[minor + ' ON PROMO'] = (subset2['EXP']*subset2['ON PROMO']).sum()/subset2['EXP'].sum()\n",
    "        \n",
    "    data.append(row)\n",
    "    \n",
    "df16_agg_minor = pd.DataFrame(data)\n",
    "\n",
    "df16_agg_minor.to_excel('df16_agg_minor.xlsx', index=False)\n",
    "del df16_agg_minor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16811612.299999975\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "df15 = pd.read_csv('2015/TGI_DATA_201501_201513.CSV', header = None, dtype = {0:object})\n",
    "\n",
    "# rename columns\n",
    "df15.columns = ['BARCODE', 'GENERATION', 'HHID', 'SHOP', 'TRANS DATE', 'TRANS TIME',\n",
    "               'QTY', 'PRICE', 'EP', 'ON PROMO', 'PLU', 'WGT']\n",
    "\n",
    "# exclude households with inconsistent records - second line excludes HH without HH SZ\n",
    "df15 = df15[df15['HHID'].isin(df15pf['HHID'])]\n",
    "df15 = df15[df15['HHID'].isin(demog15['HHID'])]\n",
    "\n",
    "# exclude non-foods by inner joining with food barcode lookup dataset\n",
    "df15 = df15.merge(bl[['BARCODE', 'SIZE', 'UOM', 'FS Major category', 'cat']], on = 'BARCODE', how = 'inner')\n",
    "\n",
    "# exclude purchases with PRICE of $0\n",
    "df15 = df15[df15['PRICE'] != 0]\n",
    "\n",
    "# adjust for extreme purchases\n",
    "df15['QTY 2'] = df15['QTY']*df15['EP']\n",
    "\n",
    "# determine the quarter of each transaction\n",
    "df15['MONTH'] = df15['TRANS DATE'].str.split('/').str[1]\n",
    "df15['YEAR'] = df15['TRANS DATE'].str.split('/').str[2]\n",
    "df15['QTR'] = np.NaN\n",
    "df15['QTR'] = np.where(df15['MONTH'].isin(['01', '02', '03']), 1, df15['QTR'])\n",
    "df15['QTR'] = np.where(df15['MONTH'].isin(['04', '05', '06']), 2, df15['QTR'])\n",
    "df15['QTR'] = np.where(df15['MONTH'].isin(['07', '08', '09']), 3, df15['QTR'])\n",
    "df15['QTR'] = np.where(df15['MONTH'].isin(['10', '11', '12']), 4, df15['QTR'])\n",
    "df15['QTR'] = np.where(df15['YEAR'] == '2014', 1, df15['QTR'])\n",
    "\n",
    "# adjust for inflation using quarterly CPI\n",
    "df15['PRICE 2'] = np.NaN\n",
    "df15['PRICE 2'] = np.where(df15['QTR'] == 1, df15['PRICE']*(109/104.1), df15['PRICE 2'])\n",
    "df15['PRICE 2'] = np.where(df15['QTR'] == 2, df15['PRICE']*(109/103.9), df15['PRICE 2'])\n",
    "df15['PRICE 2'] = np.where(df15['QTR'] == 3, df15['PRICE']*(109/104.0), df15['PRICE 2'])\n",
    "df15['PRICE 2'] = np.where(df15['QTR'] == 4, df15['PRICE']*(109/104.3), df15['PRICE 2'])\n",
    "\n",
    "# create clean size column\n",
    "df15['SIZE 2'] = np.where(df15['UOM'].isin(['GM', 'ML']), df15['SIZE'], np.NaN)\n",
    "\n",
    "# create columns for expenditure and purchase volume\n",
    "df15['EXP'] = df15['PRICE 2']*df15['QTY 2']\n",
    "df15['PUR VOL'] = df15['SIZE 2']*df15['QTY 2']\n",
    "\n",
    "print(df15['QTY 2'].sum())\n",
    "\n",
    "# isolate required columns\n",
    "df15 = df15[['HHID', 'FS Major category', 'cat', 'EXP', 'PUR VOL', 'ON PROMO', 'QTY 2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "HHIDs = df15['HHID'].unique().tolist()\n",
    "for HHID in HHIDs:\n",
    "    subset = df15[df15['HHID'] == HHID]\n",
    "    row = {}\n",
    "    row[' HHID'] = HHID\n",
    "    row[' YEAR'] = '2015'\n",
    "    tot_exp = subset['EXP'].sum()\n",
    "    row['TOT EXP'] = tot_exp\n",
    "    for major in majors:\n",
    "        subset2 = subset[subset['FS Major category'] == major]\n",
    "        row[major + ' SHARE'] = subset2['EXP'].sum()/tot_exp\n",
    "        subset3 = subset2[~subset2['PUR VOL'].isnull()]\n",
    "        row[major + ' PRICE'] = (1000)*(subset3['EXP'].sum()/subset3['PUR VOL'].sum())\n",
    "        \n",
    "    data.append(row)\n",
    "    \n",
    "df15_agg = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15_agg.to_excel('df15_agg.xlsx', index=False)\n",
    "del df15_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "HHIDs = df15['HHID'].unique().tolist()\n",
    "for HHID in HHIDs:\n",
    "    subset = df15[df15['HHID'] == HHID]\n",
    "    row = {}\n",
    "    row[' HHID'] = HHID\n",
    "    row[' YEAR'] = '2015'\n",
    "    tot_exp = subset['EXP'].sum()\n",
    "    row['TOT EXP'] = tot_exp\n",
    "    for minor in minors:\n",
    "        subset2 = subset[subset['cat'] == minor]\n",
    "        row[minor + ' SHARE'] = subset2['EXP'].sum()/tot_exp\n",
    "        subset3 = subset2[~subset2['PUR VOL'].isnull()]\n",
    "        row[minor + ' PRICE'] = (1000)*(subset3['EXP'].sum()/subset3['PUR VOL'].sum())\n",
    "        row[minor + ' ON PROMO'] = (subset2['EXP']*subset2['ON PROMO']).sum()/subset2['EXP'].sum()\n",
    "        \n",
    "    data.append(row)\n",
    "    \n",
    "df15_agg_minor = pd.DataFrame(data)\n",
    "\n",
    "df15_agg_minor.to_excel('df15_agg_minor.xlsx', index=False)\n",
    "del df15_agg_minor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAJOR CATEGORY PREP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15_agg = pd.read_excel('df15_agg.xlsx')\n",
    "df16_agg = pd.read_excel('df16_agg.xlsx')\n",
    "df17_agg = pd.read_excel('df17_agg.xlsx')\n",
    "df18_agg = pd.read_excel('df18_agg.xlsx')\n",
    "df19_agg = pd.read_excel('df19_agg.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1575\n",
       "2    1148\n",
       "3    1168\n",
       "4    1263\n",
       "5    4854\n",
       "Name:  HHID, dtype: int64"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df15_agg[[' HHID']], \n",
    "           df16_agg[[' HHID']],\n",
    "           df17_agg[[' HHID']],\n",
    "           df18_agg[[' HHID']],\n",
    "           df19_agg[[' HHID']]])[' HHID'].value_counts().value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10008"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df15_agg[[' HHID']], \n",
    "           df16_agg[[' HHID']],\n",
    "           df17_agg[[' HHID']],\n",
    "           df18_agg[[' HHID']],\n",
    "           df19_agg[[' HHID']]])[' HHID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36697"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = df15_agg.shape[0]\n",
    "w = df16_agg.shape[0]\n",
    "e = df17_agg.shape[0]\n",
    "r = df18_agg.shape[0]\n",
    "t = df19_agg.shape[0]\n",
    "\n",
    "q+w+e+r+t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36697"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = df15_agg.shape[0]\n",
    "w = df16_agg.shape[0]\n",
    "e = df17_agg.shape[0]\n",
    "r = df18_agg.shape[0]\n",
    "t = df19_agg.shape[0]\n",
    "\n",
    "q+w+e+r+t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other                              1.000\n",
       "Dairy products and alternatives    1.000\n",
       "Meat and alternatives              1.000\n",
       "Grains                             0.999\n",
       "Confectionery and snacks           0.999\n",
       "Vegetables                         0.999\n",
       "Non-alcoholic beverages            0.999\n",
       "Fruit                              0.997\n",
       "Biscuits, cakes, and desserts      0.996\n",
       "Name: FS Major category, dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data availability\n",
    "np.round(df19[~df19['PUR VOL'].isnull()].drop_duplicates(subset = ['HHID', 'FS Major category'])['FS Major category'].value_counts()/df19['HHID'].nunique(), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dairy products and alternatives    1.000\n",
       "Meat and alternatives              1.000\n",
       "Other                              1.000\n",
       "Grains                             0.999\n",
       "Vegetables                         0.999\n",
       "Non-alcoholic beverages            0.998\n",
       "Confectionery and snacks           0.998\n",
       "Fruit                              0.997\n",
       "Biscuits, cakes, and desserts      0.997\n",
       "Name: FS Major category, dtype: float64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data availability\n",
    "np.round(df18[~df18['PUR VOL'].isnull()].drop_duplicates(subset = ['HHID', 'FS Major category'])['FS Major category'].value_counts()/df18['HHID'].nunique(), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dairy products and alternatives    1.000\n",
       "Other                              1.000\n",
       "Meat and alternatives              1.000\n",
       "Grains                             1.000\n",
       "Vegetables                         1.000\n",
       "Non-alcoholic beverages            0.999\n",
       "Fruit                              0.998\n",
       "Confectionery and snacks           0.998\n",
       "Biscuits, cakes, and desserts      0.997\n",
       "Name: FS Major category, dtype: float64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data availability\n",
    "np.round(df17[~df17['PUR VOL'].isnull()].drop_duplicates(subset = ['HHID', 'FS Major category'])['FS Major category'].value_counts()/df17['HHID'].nunique(), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dairy products and alternatives    1.000\n",
       "Other                              1.000\n",
       "Meat and alternatives              1.000\n",
       "Grains                             0.999\n",
       "Non-alcoholic beverages            0.999\n",
       "Vegetables                         0.999\n",
       "Biscuits, cakes, and desserts      0.999\n",
       "Confectionery and snacks           0.998\n",
       "Fruit                              0.998\n",
       "Name: FS Major category, dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data availability\n",
    "np.round(df16[~df16['PUR VOL'].isnull()].drop_duplicates(subset = ['HHID', 'FS Major category'])['FS Major category'].value_counts()/df16['HHID'].nunique(), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other                              1.000\n",
       "Dairy products and alternatives    1.000\n",
       "Meat and alternatives              1.000\n",
       "Grains                             1.000\n",
       "Non-alcoholic beverages            0.999\n",
       "Vegetables                         0.999\n",
       "Fruit                              0.999\n",
       "Biscuits, cakes, and desserts      0.998\n",
       "Confectionery and snacks           0.998\n",
       "Name: FS Major category, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data availability\n",
    "np.round(df15[~df15['PUR VOL'].isnull()].drop_duplicates(subset = ['HHID', 'FS Major category'])['FS Major category'].value_counts()/df15['HHID'].nunique(), 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing prices, exclude outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = pd.DataFrame(df19_agg[[x + ' PRICE' for x in majors]].median()).reset_index()\n",
    "limits = pd.DataFrame(df19_agg[[x + ' PRICE' for x in majors]].mean() + df19_agg[[x + ' PRICE' for x in majors]].std()*2.5).reset_index()\n",
    "\n",
    "# replace missing values with the median\n",
    "for major in majors:\n",
    "    df19_agg[major + ' PRICE'] = df19_agg[major + ' PRICE'].fillna(medians[medians['index'] == major + ' PRICE'][0].tolist()[0])\n",
    "    \n",
    "# replace outlying values\n",
    "for major in majors:\n",
    "    lim = limits[limits['index'] == major + ' PRICE'][0].tolist()[0]\n",
    "    #df19_agg[major + ' PRICE'] = np.where(df19_agg[major + ' PRICE'] > lim, lim, df19_agg[major + ' PRICE'])\n",
    "    #df19_agg = df19_agg[df19_agg[major + ' PRICE'] <= lim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = pd.DataFrame(df18_agg[[x + ' PRICE' for x in majors]].median()).reset_index()\n",
    "limits = pd.DataFrame(df18_agg[[x + ' PRICE' for x in majors]].mean() + df18_agg[[x + ' PRICE' for x in majors]].std()*2.5).reset_index()\n",
    "\n",
    "# replace missing values with the median\n",
    "for major in majors:\n",
    "    df18_agg[major + ' PRICE'] = df18_agg[major + ' PRICE'].fillna(medians[medians['index'] == major + ' PRICE'][0].tolist()[0])\n",
    "    \n",
    "# replace outlying values\n",
    "for major in majors:\n",
    "    lim = limits[limits['index'] == major + ' PRICE'][0].tolist()[0]\n",
    "    #df18_agg[major + ' PRICE'] = np.where(df18_agg[major + ' PRICE'] > lim, lim, df18_agg[major + ' PRICE'])\n",
    "    #df18_agg = df18_agg[df18_agg[major + ' PRICE'] <= lim]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = pd.DataFrame(df17_agg[[x + ' PRICE' for x in majors]].median()).reset_index()\n",
    "limits = pd.DataFrame(df17_agg[[x + ' PRICE' for x in majors]].mean() + df17_agg[[x + ' PRICE' for x in majors]].std()*2.5).reset_index()\n",
    "\n",
    "# replace missing values with the median\n",
    "for major in majors:\n",
    "    df17_agg[major + ' PRICE'] = df17_agg[major + ' PRICE'].fillna(medians[medians['index'] == major + ' PRICE'][0].tolist()[0])\n",
    "    \n",
    "# replace outlying values\n",
    "for major in majors:\n",
    "    lim = limits[limits['index'] == major + ' PRICE'][0].tolist()[0]\n",
    "    #df17_agg[major + ' PRICE'] = np.where(df17_agg[major + ' PRICE'] > lim, lim, df17_agg[major + ' PRICE'])\n",
    "    #df17_agg = df17_agg[df17_agg[major + ' PRICE'] <= lim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = pd.DataFrame(df16_agg[[x + ' PRICE' for x in majors]].median()).reset_index()\n",
    "limits = pd.DataFrame(df16_agg[[x + ' PRICE' for x in majors]].mean() + df16_agg[[x + ' PRICE' for x in majors]].std()*2.5).reset_index()\n",
    "\n",
    "# replace missing values with the median\n",
    "for major in majors:\n",
    "    df16_agg[major + ' PRICE'] = df16_agg[major + ' PRICE'].fillna(medians[medians['index'] == major + ' PRICE'][0].tolist()[0])\n",
    "    \n",
    "# replace outlying values\n",
    "for major in majors:\n",
    "    lim = limits[limits['index'] == major + ' PRICE'][0].tolist()[0]\n",
    "    #df16_agg[major + ' PRICE'] = np.where(df16_agg[major + ' PRICE'] > lim, lim, df16_agg[major + ' PRICE'])\n",
    "    #df16_agg = df16_agg[df16_agg[major + ' PRICE'] <= lim]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = pd.DataFrame(df15_agg[[x + ' PRICE' for x in majors]].median()).reset_index()\n",
    "limits = pd.DataFrame(df15_agg[[x + ' PRICE' for x in majors]].mean() + df15_agg[[x + ' PRICE' for x in majors]].std()*2.5).reset_index()\n",
    "\n",
    "# replace missing values with the median\n",
    "for major in majors:\n",
    "    df15_agg[major + ' PRICE'] = df15_agg[major + ' PRICE'].fillna(medians[medians['index'] == major + ' PRICE'][0].tolist()[0])\n",
    "    \n",
    "# replace outlying values\n",
    "for major in majors:\n",
    "    lim = limits[limits['index'] == major + ' PRICE'][0].tolist()[0]\n",
    "    #df15_agg[major + ' PRICE'] = np.where(df15_agg[major + ' PRICE'] > lim, lim, df15_agg[major + ' PRICE'])\n",
    "    #df15_agg = df15_agg[df15_agg[major + ' PRICE'] <= lim]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge household size and SEIFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "df19_agg = df19_agg.merge(demog19[['HHID', 'HH SZ', 'Children', 'Adolescent and adult men', 'Adolescent and adult women', 'INCOME_VAL']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(seifa19[['HHID', 'SES']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(df19pf[['HHID', 'REGION']], left_on = ' HHID', right_on = 'HHID', how = 'left')\n",
    "\n",
    "df18_agg = df18_agg.merge(demog18[['HHID', 'HH SZ', 'Children', 'Adolescent and adult men', 'Adolescent and adult women', 'INCOME_VAL']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(seifa18[['HHID', 'SES']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(df18pf[['HHID', 'REGION']], left_on = ' HHID', right_on = 'HHID', how = 'left')\n",
    "\n",
    "df17_agg = df17_agg.merge(demog17[['HHID', 'HH SZ', 'Children', 'Adolescent and adult men', 'Adolescent and adult women', 'INCOME_VAL']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(seifa17[['HHID', 'SES']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(df17pf[['HHID', 'REGION']], left_on = ' HHID', right_on = 'HHID', how = 'left')\n",
    "\n",
    "df16_agg = df16_agg.merge(demog16[['HHID', 'HH SZ', 'Children', 'Adolescent and adult men', 'Adolescent and adult women', 'INCOME_VAL']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(seifa16[['HHID', 'SES']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(df16pf[['HHID', 'REGION']], left_on = ' HHID', right_on = 'HHID', how = 'left')\n",
    "\n",
    "df15_agg = df15_agg.merge(demog15[['HHID', 'HH SZ', 'Children', 'Adolescent and adult men', 'Adolescent and adult women', 'INCOME_VAL']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(seifa15[['HHID', 'SES']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(df15pf[['HHID', 'REGION']], left_on = ' HHID', right_on = 'HHID', how = 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_agg = pd.concat([df19_agg, df18_agg, df17_agg, df16_agg, df15_agg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_agg[[' HHID',\n",
    " ' YEAR',\n",
    " 'SES',\n",
    "       \n",
    " 'Biscuits, cakes, and desserts PRICE',\n",
    " 'Dairy products and alternatives PRICE',\n",
    " 'Fruit PRICE',\n",
    " 'Grains PRICE',\n",
    " 'Meat and alternatives PRICE',\n",
    " 'Non-alcoholic beverages PRICE',\n",
    " 'Other PRICE',\n",
    " 'Confectionery and snacks PRICE',\n",
    " 'Vegetables PRICE',\n",
    "\n",
    " 'Biscuits, cakes, and desserts SHARE',\n",
    " 'Dairy products and alternatives SHARE',\n",
    " 'Fruit SHARE',\n",
    " 'Grains SHARE',\n",
    " 'Meat and alternatives SHARE',\n",
    " 'Non-alcoholic beverages SHARE',\n",
    " 'Other SHARE',\n",
    " 'Confectionery and snacks SHARE',\n",
    " 'Vegetables SHARE',                \n",
    " \n",
    " 'TOT EXP',\n",
    " 'INCOME_VAL',              \n",
    " 'Children', \n",
    " 'Adolescent and adult men', \n",
    " 'Adolescent and adult women',\n",
    " 'HH SZ',\n",
    "                \n",
    " 'REGION']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg['REGION 2'] = df_agg['REGION'].replace(1, 'SYD').replace(2, 'MEL').replace(3, 'BRI').replace(4, 'ADE').replace(5, 'PER').replace(6, 'Other').replace(7, 'Other').replace(8, 'Other').replace(9, 'Other').replace(10, 'Other').replace(11, 'Other').replace(12, 'Other').replace(13, 'Other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPI = pd.read_csv('FI2Vegetables.csv')\n",
    "FPI.columns = ['YEAR-HHID', 'Vegetables FPI']\n",
    "\n",
    "for major in majors[1:]:\n",
    "    FPI[major + ' FPI'] = pd.read_csv('FI2' + major + '.csv')['Fisher Price Index']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg['YEAR-HHID'] = df_agg[' YEAR'].astype(str) + '|' + df_agg[' HHID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     36208\n",
       "False      489\n",
       "Name: YEAR-HHID, dtype: int64"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg['YEAR-HHID'].isin(FPI['YEAR-HHID']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_agg.merge(FPI, on = 'YEAR-HHID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_agg[~df_agg['Other FPI'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.to_excel('df_agg_June.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINOR CATEGORY PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate missingness\n",
    "\n",
    "df15_agg_minor = pd.read_excel('df15_agg_minor.xlsx')\n",
    "df16_agg_minor = pd.read_excel('df16_agg_minor.xlsx')\n",
    "df17_agg_minor = pd.read_excel('df17_agg_minor.xlsx')\n",
    "df18_agg_minor = pd.read_excel('df18_agg_minor.xlsx')\n",
    "df19_agg_minor = pd.read_excel('df19_agg_minor.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1575\n",
       "2    1148\n",
       "3    1168\n",
       "4    1263\n",
       "5    4854\n",
       "Name:  HHID, dtype: int64"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df15_agg_minor[[' HHID']], \n",
    "           df16_agg_minor[[' HHID']],\n",
    "           df17_agg_minor[[' HHID']],\n",
    "           df18_agg_minor[[' HHID']],\n",
    "           df19_agg_minor[[' HHID']]])[' HHID'].value_counts().value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10008"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df15_agg_minor[[' HHID']], \n",
    "           df16_agg_minor[[' HHID']],\n",
    "           df17_agg_minor[[' HHID']],\n",
    "           df18_agg_minor[[' HHID']],\n",
    "           df19_agg_minor[[' HHID']]])[' HHID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36697"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = df15_agg_minor.shape[0]\n",
    "w = df16_agg_minor.shape[0]\n",
    "e = df17_agg_minor.shape[0]\n",
    "r = df18_agg_minor.shape[0]\n",
    "t = df19_agg_minor.shape[0]\n",
    "\n",
    "q+w+e+r+t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vegetables                              0.999\n",
       "Sauces, dressings, spreads, and dips    0.998\n",
       "Fruit                                   0.997\n",
       "Milk                                    0.994\n",
       "Cheese and cream                        0.993\n",
       "Ready meals                             0.992\n",
       "Pasta, rice, and other cereals          0.992\n",
       "Biscuits                                0.992\n",
       "Fats and oils                           0.991\n",
       "Bread                                   0.991\n",
       "Processed meat                          0.990\n",
       "Chocolate-based confectionery           0.985\n",
       "Snackfoods                              0.977\n",
       "Red meat                                0.965\n",
       "Poultry                                 0.965\n",
       "Tea and coffee                          0.962\n",
       "Legumes/beans                           0.958\n",
       "Sugar-sweetened beverages               0.949\n",
       "Fish and seafoods                       0.948\n",
       "Non-sugar sweetened beverages           0.947\n",
       "Breakfast cereals                       0.947\n",
       "Cakes, muffins, and pastries            0.944\n",
       "Ice-cream                               0.941\n",
       "Sugars, honey, and related products     0.927\n",
       "Sugar-based confectionery               0.927\n",
       "Yoghurt                                 0.908\n",
       "Eggs                                    0.890\n",
       "Nuts and seeds                          0.874\n",
       "Desserts                                0.828\n",
       "Name: cat, dtype: float64"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data availability\n",
    "np.round(df19[~df19['PUR VOL'].isnull()].drop_duplicates(subset = ['HHID', 'cat'])['cat'].value_counts()/df19['HHID'].nunique(), 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vegetables                              0.999\n",
       "Sauces, dressings, spreads, and dips    0.998\n",
       "Fruit                                   0.997\n",
       "Milk                                    0.996\n",
       "Cheese and cream                        0.993\n",
       "Biscuits                                0.993\n",
       "Bread                                   0.992\n",
       "Ready meals                             0.991\n",
       "Processed meat                          0.990\n",
       "Fats and oils                           0.990\n",
       "Pasta, rice, and other cereals          0.990\n",
       "Chocolate-based confectionery           0.981\n",
       "Snackfoods                              0.977\n",
       "Red meat                                0.969\n",
       "Poultry                                 0.968\n",
       "Tea and coffee                          0.964\n",
       "Legumes/beans                           0.960\n",
       "Breakfast cereals                       0.952\n",
       "Fish and seafoods                       0.951\n",
       "Sugar-sweetened beverages               0.950\n",
       "Non-sugar sweetened beverages           0.949\n",
       "Cakes, muffins, and pastries            0.942\n",
       "Ice-cream                               0.939\n",
       "Sugars, honey, and related products     0.934\n",
       "Sugar-based confectionery               0.921\n",
       "Yoghurt                                 0.906\n",
       "Eggs                                    0.899\n",
       "Nuts and seeds                          0.872\n",
       "Desserts                                0.843\n",
       "Name: cat, dtype: float64"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data availability\n",
    "np.round(df18[~df18['PUR VOL'].isnull()].drop_duplicates(subset = ['HHID', 'cat'])['cat'].value_counts()/df18['HHID'].nunique(), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vegetables                              1.000\n",
       "Fruit                                   0.998\n",
       "Sauces, dressings, spreads, and dips    0.998\n",
       "Milk                                    0.996\n",
       "Cheese and cream                        0.994\n",
       "Biscuits                                0.993\n",
       "Processed meat                          0.992\n",
       "Fats and oils                           0.992\n",
       "Bread                                   0.992\n",
       "Pasta, rice, and other cereals          0.990\n",
       "Ready meals                             0.989\n",
       "Chocolate-based confectionery           0.986\n",
       "Snackfoods                              0.978\n",
       "Poultry                                 0.973\n",
       "Red meat                                0.969\n",
       "Tea and coffee                          0.965\n",
       "Legumes/beans                           0.964\n",
       "Non-sugar sweetened beverages           0.958\n",
       "Breakfast cereals                       0.956\n",
       "Fish and seafoods                       0.954\n",
       "Sugar-sweetened beverages               0.952\n",
       "Ice-cream                               0.948\n",
       "Sugars, honey, and related products     0.943\n",
       "Cakes, muffins, and pastries            0.939\n",
       "Sugar-based confectionery               0.922\n",
       "Yoghurt                                 0.913\n",
       "Eggs                                    0.898\n",
       "Nuts and seeds                          0.878\n",
       "Desserts                                0.855\n",
       "Name: cat, dtype: float64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data availability\n",
    "np.round(df17[~df17['PUR VOL'].isnull()].drop_duplicates(subset = ['HHID', 'cat'])['cat'].value_counts()/df17['HHID'].nunique(), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vegetables                              0.999\n",
       "Fruit                                   0.998\n",
       "Sauces, dressings, spreads, and dips    0.998\n",
       "Milk                                    0.997\n",
       "Biscuits                                0.995\n",
       "Fats and oils                           0.995\n",
       "Cheese and cream                        0.994\n",
       "Pasta, rice, and other cereals          0.993\n",
       "Bread                                   0.993\n",
       "Processed meat                          0.992\n",
       "Ready meals                             0.991\n",
       "Chocolate-based confectionery           0.985\n",
       "Snackfoods                              0.977\n",
       "Poultry                                 0.974\n",
       "Red meat                                0.973\n",
       "Legumes/beans                           0.967\n",
       "Tea and coffee                          0.963\n",
       "Non-sugar sweetened beverages           0.961\n",
       "Breakfast cereals                       0.958\n",
       "Fish and seafoods                       0.957\n",
       "Sugar-sweetened beverages               0.956\n",
       "Sugars, honey, and related products     0.949\n",
       "Ice-cream                               0.947\n",
       "Cakes, muffins, and pastries            0.947\n",
       "Sugar-based confectionery               0.930\n",
       "Yoghurt                                 0.915\n",
       "Eggs                                    0.891\n",
       "Nuts and seeds                          0.876\n",
       "Desserts                                0.863\n",
       "Name: cat, dtype: float64"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data availability\n",
    "np.round(df16[~df16['PUR VOL'].isnull()].drop_duplicates(subset = ['HHID', 'cat'])['cat'].value_counts()/df16['HHID'].nunique(), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sauces, dressings, spreads, and dips    0.999\n",
       "Vegetables                              0.999\n",
       "Fruit                                   0.999\n",
       "Milk                                    0.997\n",
       "Processed meat                          0.995\n",
       "Biscuits                                0.995\n",
       "Cheese and cream                        0.995\n",
       "Bread                                   0.993\n",
       "Fats and oils                           0.993\n",
       "Pasta, rice, and other cereals          0.993\n",
       "Ready meals                             0.992\n",
       "Chocolate-based confectionery           0.983\n",
       "Red meat                                0.977\n",
       "Poultry                                 0.974\n",
       "Snackfoods                              0.974\n",
       "Legumes/beans                           0.973\n",
       "Tea and coffee                          0.966\n",
       "Non-sugar sweetened beverages           0.961\n",
       "Sugar-sweetened beverages               0.960\n",
       "Breakfast cereals                       0.958\n",
       "Fish and seafoods                       0.957\n",
       "Sugars, honey, and related products     0.951\n",
       "Ice-cream                               0.943\n",
       "Cakes, muffins, and pastries            0.942\n",
       "Sugar-based confectionery               0.929\n",
       "Yoghurt                                 0.919\n",
       "Eggs                                    0.881\n",
       "Nuts and seeds                          0.871\n",
       "Desserts                                0.864\n",
       "Name: cat, dtype: float64"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data availability\n",
    "np.round(df15[~df15['PUR VOL'].isnull()].drop_duplicates(subset = ['HHID', 'cat'])['cat'].value_counts()/df15['HHID'].nunique(), 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE TO FUTURE SELF: if you present unit prices in the manuscript, need to consider that some have been imputed, perhaps don't present the imputed ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Impute missing prices, exclude outliers\n",
    "\n",
    "medians = pd.DataFrame(df19_agg_minor[[x + ' PRICE' for x in minors]].median()).reset_index()\n",
    "limits = pd.DataFrame(df19_agg_minor[[x + ' PRICE' for x in minors]].mean() + df19_agg_minor[[x + ' PRICE' for x in minors]].std()*2.5).reset_index()\n",
    "\n",
    "# replace missing values with the median\n",
    "for minor in minors:\n",
    "    df19_agg_minor[minor + ' PRICE'] = df19_agg_minor[minor + ' PRICE'].fillna(medians[medians['index'] == minor + ' PRICE'][0].tolist()[0])\n",
    "    \n",
    "# replace outlying values\n",
    "for minor in minors:\n",
    "    lim = limits[limits['index'] == minor + ' PRICE'][0].tolist()[0]\n",
    "    #df19_agg_minor[minor + ' PRICE'] = np.where(df19_agg_minor[minor + ' PRICE'] > lim, lim, df19_agg_minor[minor + ' PRICE'])\n",
    "    #df19_agg_minor = df19_agg_minor[df19_agg_minor[minor + ' PRICE'] <= lim]\n",
    "\n",
    "\n",
    "medians = pd.DataFrame(df18_agg_minor[[x + ' PRICE' for x in minors]].median()).reset_index()\n",
    "limits = pd.DataFrame(df18_agg_minor[[x + ' PRICE' for x in minors]].mean() + df18_agg_minor[[x + ' PRICE' for x in minors]].std()*2.5).reset_index()\n",
    "\n",
    "# replace missing values with the median\n",
    "for minor in minors:\n",
    "    df18_agg_minor[minor + ' PRICE'] = df18_agg_minor[minor + ' PRICE'].fillna(medians[medians['index'] == minor + ' PRICE'][0].tolist()[0])\n",
    "    \n",
    "# replace outlying values\n",
    "for minor in minors:\n",
    "    lim = limits[limits['index'] == minor + ' PRICE'][0].tolist()[0]\n",
    "    #df18_agg_minor[minor + ' PRICE'] = np.where(df18_agg_minor[minor + ' PRICE'] > lim, lim, df18_agg_minor[minor + ' PRICE'])\n",
    "    #df18_agg_minor = df18_agg_minor[df18_agg_minor[minor + ' PRICE'] <= lim]\n",
    "    \n",
    "\n",
    "medians = pd.DataFrame(df17_agg_minor[[x + ' PRICE' for x in minors]].median()).reset_index()\n",
    "limits = pd.DataFrame(df17_agg_minor[[x + ' PRICE' for x in minors]].mean() + df17_agg_minor[[x + ' PRICE' for x in minors]].std()*2.5).reset_index()\n",
    "\n",
    "# replace missing values with the median\n",
    "for minor in minors:\n",
    "    df17_agg_minor[minor + ' PRICE'] = df17_agg_minor[minor + ' PRICE'].fillna(medians[medians['index'] == minor + ' PRICE'][0].tolist()[0])\n",
    "    \n",
    "# replace outlying values\n",
    "for minor in minors:\n",
    "    lim = limits[limits['index'] == minor + ' PRICE'][0].tolist()[0]\n",
    "    #df17_agg_minor[minor + ' PRICE'] = np.where(df17_agg_minor[minor + ' PRICE'] > lim, lim, df17_agg_minor[minor + ' PRICE'])\n",
    "    #df17_agg_minor = df17_agg_minor[df17_agg_minor[minor + ' PRICE'] <= lim]\n",
    "\n",
    "\n",
    "medians = pd.DataFrame(df16_agg_minor[[x + ' PRICE' for x in minors]].median()).reset_index()\n",
    "limits = pd.DataFrame(df16_agg_minor[[x + ' PRICE' for x in minors]].mean() + df16_agg_minor[[x + ' PRICE' for x in minors]].std()*2.5).reset_index()\n",
    "\n",
    "# replace missing values with the median\n",
    "for minor in minors:\n",
    "    df16_agg_minor[minor + ' PRICE'] = df16_agg_minor[minor + ' PRICE'].fillna(medians[medians['index'] == minor + ' PRICE'][0].tolist()[0])\n",
    "    \n",
    "# replace outlying values\n",
    "for minor in minors:\n",
    "    lim = limits[limits['index'] == minor + ' PRICE'][0].tolist()[0]\n",
    "    #df16_agg_minor[minor + ' PRICE'] = np.where(df16_agg_minor[minor + ' PRICE'] > lim, lim, df16_agg_minor[minor + ' PRICE'])\n",
    "    #df16_agg_minor = df16_agg_minor[df16_agg_minor[minor + ' PRICE'] <= lim]\n",
    "    \n",
    "\n",
    "medians = pd.DataFrame(df15_agg_minor[[x + ' PRICE' for x in minors]].median()).reset_index()\n",
    "limits = pd.DataFrame(df15_agg_minor[[x + ' PRICE' for x in minors]].mean() + df15_agg_minor[[x + ' PRICE' for x in minors]].std()*2.5).reset_index()\n",
    "\n",
    "# replace missing values with the median\n",
    "for minor in minors:\n",
    "    df15_agg_minor[minor + ' PRICE'] = df15_agg_minor[minor + ' PRICE'].fillna(medians[medians['index'] == minor + ' PRICE'][0].tolist()[0])\n",
    "    \n",
    "# replace outlying values\n",
    "for minor in minors:\n",
    "    lim = limits[limits['index'] == minor + ' PRICE'][0].tolist()[0]\n",
    "    #df15_agg_minor[minor + ' PRICE'] = np.where(df15_agg_minor[minor + ' PRICE'] > lim, lim, df15_agg_minor[minor + ' PRICE'])\n",
    "    #df15_agg_minor = df15_agg_minor[df15_agg_minor[minor + ' PRICE'] <= lim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge household size and SEIFA\n",
    "\n",
    "df19_agg_minor = df19_agg_minor.merge(demog19[['HHID', 'HH SZ', 'Children', 'Adolescent and adult men', 'Adolescent and adult women', 'INCOME_VAL']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(seifa19[['HHID', 'SES']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(df19pf[['HHID', 'REGION']], left_on = ' HHID', right_on = 'HHID', how = 'left')\n",
    "\n",
    "df18_agg_minor = df18_agg_minor.merge(demog18[['HHID', 'HH SZ', 'Children', 'Adolescent and adult men', 'Adolescent and adult women', 'INCOME_VAL']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(seifa18[['HHID', 'SES']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(df18pf[['HHID', 'REGION']], left_on = ' HHID', right_on = 'HHID', how = 'left')\n",
    "\n",
    "df17_agg_minor = df17_agg_minor.merge(demog17[['HHID', 'HH SZ', 'Children', 'Adolescent and adult men', 'Adolescent and adult women', 'INCOME_VAL']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(seifa17[['HHID', 'SES']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(df17pf[['HHID', 'REGION']], left_on = ' HHID', right_on = 'HHID', how = 'left')\n",
    "\n",
    "df16_agg_minor = df16_agg_minor.merge(demog16[['HHID', 'HH SZ', 'Children', 'Adolescent and adult men', 'Adolescent and adult women', 'INCOME_VAL']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(seifa16[['HHID', 'SES']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(df16pf[['HHID', 'REGION']], left_on = ' HHID', right_on = 'HHID', how = 'left')\n",
    "\n",
    "df15_agg_minor = df15_agg_minor.merge(demog15[['HHID', 'HH SZ', 'Children', 'Adolescent and adult men', 'Adolescent and adult women', 'INCOME_VAL']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(seifa15[['HHID', 'SES']], left_on = ' HHID', right_on = 'HHID', how = 'left').merge(df15pf[['HHID', 'REGION']], left_on = ' HHID', right_on = 'HHID', how = 'left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_minor = pd.concat([df19_agg_minor, df18_agg_minor, df17_agg_minor, df16_agg_minor, df15_agg_minor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36697, 100)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg_minor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_minor = df_agg_minor[[' HHID',\n",
    " ' YEAR',\n",
    " 'SES',\n",
    "       \n",
    " 'Biscuits PRICE',\n",
    " 'Bread PRICE',\n",
    " 'Breakfast cereals PRICE',\n",
    " 'Cakes, muffins, and pastries PRICE',\n",
    " 'Cheese and cream PRICE',\n",
    " 'Chocolate-based confectionery PRICE',\n",
    " 'Desserts PRICE',\n",
    " 'Eggs PRICE',\n",
    " 'Fats and oils PRICE',\n",
    " 'Fish and seafoods PRICE',\n",
    " 'Fruit PRICE',\n",
    " 'Ice-cream PRICE',\n",
    " 'Legumes/beans PRICE',\n",
    " 'Milk PRICE',\n",
    " 'Non-sugar sweetened beverages PRICE',\n",
    " 'Nuts and seeds PRICE',\n",
    " 'Pasta, rice, and other cereals PRICE',\n",
    " 'Poultry PRICE',\n",
    " 'Processed meat PRICE',\n",
    " 'Ready meals PRICE',\n",
    " 'Red meat PRICE',\n",
    " 'Sauces, dressings, spreads, and dips PRICE',\n",
    " 'Snackfoods PRICE',\n",
    " 'Sugar-based confectionery PRICE',\n",
    " 'Sugar-sweetened beverages PRICE',\n",
    " 'Sugars, honey, and related products PRICE',\n",
    " 'Tea and coffee PRICE',\n",
    " 'Vegetables PRICE',\n",
    " 'Yoghurt PRICE',\n",
    "\n",
    " 'Biscuits SHARE',\n",
    " 'Bread SHARE',\n",
    " 'Breakfast cereals SHARE',\n",
    " 'Cakes, muffins, and pastries SHARE',\n",
    " 'Cheese and cream SHARE',\n",
    " 'Chocolate-based confectionery SHARE',\n",
    " 'Desserts SHARE',\n",
    " 'Eggs SHARE',\n",
    " 'Fats and oils SHARE',\n",
    " 'Fish and seafoods SHARE',\n",
    " 'Fruit SHARE',\n",
    " 'Ice-cream SHARE',\n",
    " 'Legumes/beans SHARE',\n",
    " 'Milk SHARE',\n",
    " 'Non-sugar sweetened beverages SHARE',\n",
    " 'Nuts and seeds SHARE',\n",
    " 'Pasta, rice, and other cereals SHARE',\n",
    " 'Poultry SHARE',\n",
    " 'Processed meat SHARE',\n",
    " 'Ready meals SHARE',\n",
    " 'Red meat SHARE',\n",
    " 'Sauces, dressings, spreads, and dips SHARE',\n",
    " 'Snackfoods SHARE',\n",
    " 'Sugar-based confectionery SHARE',\n",
    " 'Sugar-sweetened beverages SHARE',\n",
    " 'Sugars, honey, and related products SHARE',\n",
    " 'Tea and coffee SHARE',\n",
    " 'Vegetables SHARE',\n",
    " 'Yoghurt SHARE',               \n",
    " \n",
    " 'TOT EXP',\n",
    " 'INCOME_VAL',              \n",
    " 'Children', \n",
    " 'Adolescent and adult men', \n",
    " 'Adolescent and adult women',\n",
    " 'HH SZ',\n",
    "                \n",
    " 'REGION']]\n",
    "\n",
    "df_agg_minor['REGION 2'] = df_agg_minor['REGION'].replace(1, 'SYD').replace(2, 'MEL').replace(3, 'BRI').replace(4, 'ADE').replace(5, 'PER').replace(6, 'Other').replace(7, 'Other').replace(8, 'Other').replace(9, 'Other').replace(10, 'Other').replace(11, 'Other').replace(12, 'Other').replace(13, 'Other')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU WILL HAVE TO CHANGE THIS\n",
    "FPI = pd.read_csv('FI2Vegetables.csv')\n",
    "FPI.columns = ['YEAR-HHID', 'Vegetables FPI']\n",
    "\n",
    "for major in majors:\n",
    "    FPI[major + ' FPI'] = pd.read_csv('FI2' + major + '.csv')['Fisher Price Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_minor['YEAR-HHID'] = df_agg_minor[' YEAR'].astype(str) + '|' + df_agg_minor[' HHID'].astype(str)\n",
    "\n",
    "df_agg_minor['YEAR-HHID'].isin(FPI['YEAR-HHID']).value_counts()\n",
    "\n",
    "df_agg_minor = df_agg_minor.merge(FPI, on = 'YEAR-HHID', how = 'left')\n",
    "\n",
    "df_agg_minor = df_agg_minor[~df_agg_minor['Other FPI'].isnull()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "for minor in minors:\n",
    "    df_agg_minor[minor + ' FPI'] = df_agg_minor['Vegetables FPI'].sample(frac=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_minor = df_agg_minor.merge(df_agg[['YEAR-HHID','Biscuits, cakes, and desserts SHARE',\n",
    " 'Dairy products and alternatives SHARE',\n",
    " 'Grains SHARE',\n",
    " 'Meat and alternatives SHARE',\n",
    " 'Non-alcoholic beverages SHARE',\n",
    " 'Other SHARE',\n",
    " 'Confectionery and snacks SHARE']], how = 'left', on = 'YEAR-HHID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_minor.to_excel('df_agg_minor_June.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.read_csv('FI2Vegetables.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.read_csv('FI2Grains.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.to_excel('df_agg_Sep.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.read_excel('df_agg_Sep.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " HHID                                       100.0\n",
       " YEAR                                       100.0\n",
       "Cakes and biscuits ON PROMO                   NaN\n",
       "Cakes and biscuits PRICE                    100.0\n",
       "Cakes and biscuits SHARE                    100.0\n",
       "Dairy products and alternatives ON PROMO      NaN\n",
       "Dairy products and alternatives PRICE       100.0\n",
       "Dairy products and alternatives SHARE       100.0\n",
       "Fruit ON PROMO                                NaN\n",
       "Fruit PRICE                                 100.0\n",
       "Fruit SHARE                                 100.0\n",
       "Grains ON PROMO                               NaN\n",
       "Grains PRICE                                100.0\n",
       "Grains SHARE                                100.0\n",
       "HH SZ                                       100.0\n",
       "HHID_x                                      100.0\n",
       "HHID_y                                      100.0\n",
       "Meat and alternatives ON PROMO                NaN\n",
       "Meat and alternatives PRICE                 100.0\n",
       "Meat and alternatives SHARE                 100.0\n",
       "Non-alcoholic beverages ON PROMO              NaN\n",
       "Non-alcoholic beverages PRICE               100.0\n",
       "Non-alcoholic beverages SHARE               100.0\n",
       "Other ON PROMO                                NaN\n",
       "Other PRICE                                 100.0\n",
       "Other SHARE                                 100.0\n",
       "Processed meat ON PROMO                       NaN\n",
       "Processed meat PRICE                        100.0\n",
       "Processed meat SHARE                        100.0\n",
       "SES                                         100.0\n",
       "Snacks and confectionery ON PROMO             NaN\n",
       "Snacks and confectionery PRICE              100.0\n",
       "Snacks and confectionery SHARE              100.0\n",
       "TOT EXP                                     100.0\n",
       "Vegetables ON PROMO                           NaN\n",
       "Vegetables PRICE                            100.0\n",
       "Vegetables SHARE                            100.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proportion of positive purchases\n",
    "np.round((100-100*((df19_agg.isnull().sum() + df18_agg.isnull().sum() + df17_agg.isnull().sum() + df16_agg.isnull().sum() + df15_agg.isnull().sum())/36698)), 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
