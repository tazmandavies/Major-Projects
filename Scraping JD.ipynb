{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Python code to scrape product text data and product images from the JD website. The scraper predominantly relies on the Python libraries Selenium, Requests, and BeautifulSoup. In brief, the scraper works by drilling down the category tree on the JD website. First, it accesses the major category pages (e.g., Drinks), then the minor category pages (e.g., Soft Drinks), and then the product category pages (e.g., Coco Cola 2L). The process is iterative to that each product page is accessed separately and just once. This Python code can be adapted to scrape product text data and product images from other websites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Required downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for this code to run, the following programs must be downloaded:\n",
    "\n",
    "- Anaconda (https://www.anaconda.com/products/distribution). This code is designed to run in Jupyter Notebook within Anaconda. There should be a free version of Anaconda.\n",
    "\n",
    "- Firefox browser\n",
    "\n",
    "- Gecko driver (https://github.com/mozilla/geckodriver/releases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import required libaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the standard libaries to conduct web scraping in Python. You'll likely be using these same libraries to scrape from other websites. Note, I believe all of these libraries are in-built in the Anaconda software. If you receive any errors importing any of these libraries, you can install the required library using pip install (https://datatofish.com/install-package-python-using-pip/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import selenium - this is used to automate browsing of any website. \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "# import requests - this is used to extract the HTML from the desired webpages\n",
    "import requests\n",
    "\n",
    "# import beautifulsoup - this is used to isolate the desired information from the extracted HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import regular expressions for data manipulation\n",
    "import re\n",
    "\n",
    "# import numpy for manipulation of numerical data\n",
    "import numpy as np\n",
    "\n",
    "# import pandas - this is used for dataframe manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# set the maximum number of columns/rows to display in pandas dataframe\n",
    "pd.set_option('display.max_columns', 999)\n",
    "pd.set_option('display.max_rows', 999)\n",
    "\n",
    "# import time so that we can time how long each step takes\n",
    "import time\n",
    "\n",
    "# import library to download images based on URL\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Open Firefox browser using Selenium and log into JD account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two challenges that are specific to scraping JD. These challenges may/may not be present when scraping other websites:\n",
    "\n",
    "- Sometimes, webpages can take >1 minute to fully load. This is problematic as the scraping time is proportional to the webpage loading time (e.g., scraping 1 million pages at a rate of 1 min/page would take 694 days!). It appears this slowness is present on both Australian and Chinese servers. For example, most of the time (but not all of the time) this page takes about 1 min 30 seconds to load: https://list.jd.com/list.html?cat=1320,1583. However, fortunately, the desired product information (i.e., product text and product images) typically finishes loading within 5 seconds of accessing a webpage, and therefore we can instruct the scraper to only wait 5 seconds for a webpage to load.\n",
    "\n",
    "- The full range of products is only viewable after logging into an account. The website will often default to the login page if a user tries to browse without being logged into an account (i.e., https://passport.jd.com/new/login.aspx). To overcome this, we will need to manually log into an account prior to scraping. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we specify that we don't want the web scraper to wait for the webpages to fully load\n",
    "# We therefore set the loading strategy to \"none\" instead of \"normal\"\n",
    "# Later in the codebase, we will instruct the scraper to wait 5 seconds after loading each page\n",
    "capa = DesiredCapabilities.FIREFOX\n",
    "capa[\"pageLoadStrategy\"] = \"none\" # here we specify for the \n",
    "\n",
    "# We then open a new firefox browser using Selenium. For this to open, make sure to:\n",
    "# (1) Update the 'executable_path' to wherever you saved geckodriver\n",
    "# (2) Ensure privacy settings in system preferences allow 'geckodriver' to be opened\n",
    "driver = webdriver.Firefox(executable_path = \"/Users/tazmandavies/Downloads/geckodriver\",\n",
    "                           desired_capabilities=capa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the browser has opened, type and enter 'https://passport.jd.com/new/login.aspx' into the \n",
    "browser search bar, and then log into a JD account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create spreadsheet of subcategory URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to manually feed the web scraper with the URLs for each JD category page. There are seven of them: 进口食品, 地方特产, 休闲食品, 粮油调味, 饮料冲调, 节庆食品/礼券, and 茗茶. For each category, we instruct the web scraper to construct a spreadsheet of subcategory URLs, as shown in steps 3.1 to 3.8. Then, as shown in step 3.9, we access each subcategory page and determine the number of products. The output from this phase is saves as 'JD_subcategory_URLs.xlsx'. When Tazman ran this code on 2022/12/18, there were 660 subcategories and 2,869,885 products! \n",
    "\n",
    "This section should take about 30-40 minutes to run.\n",
    "\n",
    "If some of the Python code doesn't make sense in this section, check out this webpage: https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. 进口食品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set category URL for 进口食品\n",
    "进口食品 = 'https://list.jd.com/list.html?cat=1320,5019'\n",
    "\n",
    "# access URL in firefox\n",
    "driver.get(进口食品)\n",
    "\n",
    "# wait 5 seconds to make sure the desired information has loaded\n",
    "time.sleep(5)\n",
    "\n",
    "# extract the HTML from the category page\n",
    "soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# create an empty dataframe (df) for 进口食品\n",
    "进口食品_df = pd.DataFrame()\n",
    "\n",
    "# add a column for the subcategory name\n",
    "进口食品_df['Subcategory'] = [x.find('a').get('onclick').split(',')[-1][1:-2] for x in soup.find_all('div', {\"class\":\"sl-value\"})[3].find_all('li')]\n",
    "\n",
    "# add a column for the subcategory URL\n",
    "进口食品_df['URL'] = ['https://search.jd.com/' + x.find('a').get('href') for x in soup.find_all('div', {\"class\":\"sl-value\"})[3].find_all('li')]\n",
    "\n",
    "# add a column for the category\n",
    "进口食品_df['Category'] = '进口食品'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. 地方特产"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set category URL for 地方特产\n",
    "地方特产 = 'https://list.jd.com/list.html?cat=1320,1581'\n",
    "\n",
    "# access URL in firefox\n",
    "driver.get(地方特产)\n",
    "\n",
    "# wait 5 seconds to make sure the desired information has loaded\n",
    "time.sleep(5)\n",
    "\n",
    "# extract the HTML from the category page\n",
    "soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# create a dataframe (df) for 地方特产\n",
    "地方特产_df = pd.DataFrame()\n",
    "地方特产_df['Subcategory'] = [x.find('a').get('onclick').split(',')[-1][1:-2] for x in soup.find_all('div', {\"class\":\"sl-value\"})[2].find_all('li')]\n",
    "地方特产_df['URL'] = ['https://search.jd.com/' + x.find('a').get('href') for x in soup.find_all('div', {\"class\":\"sl-value\"})[2].find_all('li')]\n",
    "地方特产_df['Category'] = '地方特产'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. 休闲食品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set category URL for 休闲食品\n",
    "休闲食品 = 'https://list.jd.com/list.html?cat=1320,1583'\n",
    "\n",
    "# access URL in firefox\n",
    "driver.get(休闲食品)\n",
    "\n",
    "# wait 5 seconds to make sure the desired information has loaded\n",
    "time.sleep(5)\n",
    "\n",
    "# extract the HTML from the category page\n",
    "soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# create a dataframe (df) for 休闲食品\n",
    "休闲食品_df = pd.DataFrame()\n",
    "休闲食品_df['Subcategory'] = [x.find('a').get('onclick').split(',')[-1][1:-2] for x in soup.find_all('li', {\"data-group\":\"1\"})]\n",
    "休闲食品_df['URL'] = ['https://search.jd.com/' + x.find('a').get('href') for x in soup.find_all('li', {\"data-group\":\"1\"})]\n",
    "休闲食品_df['Category'] = '休闲食品'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. 粮油调味"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set category URL for 粮油调味\n",
    "粮油调味 = 'https://list.jd.com/list.html?cat=1320,1584'\n",
    "\n",
    "# access URL in firefox\n",
    "driver.get(粮油调味)\n",
    "\n",
    "# wait 5 seconds to make sure the desired information has loaded\n",
    "time.sleep(5)\n",
    "\n",
    "# extract the HTML from the category page\n",
    "soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# create a dataframe (df) for 粮油调味\n",
    "粮油调味_df = pd.DataFrame()\n",
    "粮油调味_df['Subcategory'] = [x.find('a').get('onclick').split(',')[-1][1:-2] for x in soup.find_all('li', {\"data-group\":\"2\"})]\n",
    "粮油调味_df['URL'] = ['https://search.jd.com/' + x.find('a').get('href') for x in soup.find_all('li', {\"data-group\":\"2\"})]\n",
    "粮油调味_df['Category'] = '粮油调味'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5. 饮料冲调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set category URL for 饮料冲调\n",
    "饮料冲调 = 'https://list.jd.com/list.html?cat=1320,1585'\n",
    "\n",
    "# access URL in firefox\n",
    "driver.get(饮料冲调)\n",
    "\n",
    "# wait 5 seconds to make sure the desired information has loaded\n",
    "time.sleep(5)\n",
    "\n",
    "# extract the HTML from the category page\n",
    "soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# create a dataframe (df) for 饮料冲调\n",
    "饮料冲调_df = pd.DataFrame()\n",
    "饮料冲调_df['Subcategory'] = [x.find('a').get('onclick').split(',')[-1][1:-2] for x in soup.find_all('li', {\"data-group\":\"6\"})]\n",
    "饮料冲调_df['URL'] = ['https://search.jd.com/' + x.find('a').get('href') for x in soup.find_all('li', {\"data-group\":\"6\"})]\n",
    "饮料冲调_df['Category'] = '饮料冲调'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6. 节庆食品/礼券"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set category URL for 节庆食品/礼券\n",
    "节庆食品 = 'https://search.jd.com/list.html?cat=1320,2641'\n",
    "\n",
    "# access URL in firefox\n",
    "driver.get(节庆食品)\n",
    "\n",
    "# wait 5 seconds to make sure the desired information has loaded\n",
    "time.sleep(5)\n",
    "\n",
    "# extract the HTML from the category page\n",
    "soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# from the HTML, extract the names and URLs of all subcategories\n",
    "节庆食品_df = pd.DataFrame()\n",
    "节庆食品_df['Subcategory'] = [x.find('a').get('onclick').split(',')[-1][1:-2] for x in soup.find_all('li', {\"data-group\":\"3\"})]\n",
    "节庆食品_df['URL'] = ['https://search.jd.com/' + x.find('a').get('href') for x in soup.find_all('li', {\"data-group\":\"3\"})]\n",
    "节庆食品_df['Category'] = '茗茶'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7. 茗茶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set category URL for 地方特产\n",
    "茗茶 = 'https://list.jd.com/list.html?cat=1320,12202'\n",
    "\n",
    "# access URL in firefox\n",
    "driver.get(茗茶)\n",
    "\n",
    "# wait 5 seconds to make sure the desired information has loaded\n",
    "time.sleep(5)\n",
    "\n",
    "# extract the HTML from the 茗茶 page\n",
    "soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# create a dataframe (df) for 茗茶\n",
    "茗茶_df = pd.DataFrame()\n",
    "茗茶_df['Subcategory'] = [x.find('a').get('onclick').split(',')[-1][1:-2] for x in soup.find_all('li', {\"data-group\":\"2\"})]\n",
    "茗茶_df['URL'] = ['https://search.jd.com/' + x.find('a').get('href') for x in soup.find_all('li', {\"data-group\":\"2\"})]\n",
    "茗茶_df['Category'] = '茗茶'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8. Combine seven dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vertically combine the seven datasets\n",
    "df = pd.concat([进口食品_df, 地方特产_df, 休闲食品_df, 粮油调味_df, 饮料冲调_df, 节庆食品_df, 茗茶_df]).reset_index(drop=True)\n",
    "\n",
    "# change order of columns\n",
    "df = df[['Category', 'Subcategory', 'URL']]\n",
    "\n",
    "# see total number of subcategories\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9. Determine the number of products in each subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 445 ms, total: 1min 1s\n",
      "Wall time: 35min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# this block should take about 35 minutes to run\n",
    "\n",
    "# create an empty list to append the number of products from each subcategory\n",
    "num_products_list = []\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    # access subcategory URL\n",
    "    driver.get(df['URL'].tolist()[i])\n",
    "    \n",
    "    # wait three seconds to make sure page has sufficiently loaded\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # extract the HTML from the page\n",
    "    soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    # extract number of products where available\n",
    "    try:\n",
    "        # extract the number of products from the 'soup' variable where possible\n",
    "        num_products = soup.find('div', {\"class\":\"f-result-sum\"}).text\n",
    "        \n",
    "    except:\n",
    "        # but if the number of products is not available, just set this to 'Not sure'\n",
    "        num_products = 'Not sure' # \n",
    "        \n",
    "    # append number of products to list\n",
    "    num_products_list.append(num_products)\n",
    "    \n",
    "# create new column in df dataset for number of products\n",
    "df['n'] = num_products_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "2706844\n"
     ]
    }
   ],
   "source": [
    "# create clean variable for number of products\n",
    "df['n clean'] = [int(eval(x)) for x in df['n'].str.replace('共', '').str.replace('件商品', '').str.replace('+', '').str.replace('万', '*10000').tolist()]\n",
    "\n",
    "# export dataset\n",
    "df.to_excel('JD_subcategory_URLs.xlsx', index=False)\n",
    "\n",
    "# see the total unumber of categories - 685\n",
    "print(df.shape[0])\n",
    "\n",
    "# see total number of products - 2.7 million!!!\n",
    "print(df['n clean'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>URL</th>\n",
       "      <th>n</th>\n",
       "      <th>n clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>进口食品</td>\n",
       "      <td>威化饼干</td>\n",
       "      <td>https://search.jd.com//list.html?cat=1320%2C50...</td>\n",
       "      <td>共1700+ 件商品</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>进口食品</td>\n",
       "      <td>海苔片</td>\n",
       "      <td>https://search.jd.com//list.html?cat=1320%2C50...</td>\n",
       "      <td>共300+ 件商品</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>进口食品</td>\n",
       "      <td>水晶米</td>\n",
       "      <td>https://search.jd.com//list.html?cat=1320%2C50...</td>\n",
       "      <td>共90+ 件商品</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>进口食品</td>\n",
       "      <td>果味汽水</td>\n",
       "      <td>https://search.jd.com//list.html?cat=1320%2C50...</td>\n",
       "      <td>共1300+ 件商品</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>进口食品</td>\n",
       "      <td>驼奶粉</td>\n",
       "      <td>https://search.jd.com//list.html?cat=1320%2C50...</td>\n",
       "      <td>共100+ 件商品</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category Subcategory                                                URL  \\\n",
       "0     进口食品        威化饼干  https://search.jd.com//list.html?cat=1320%2C50...   \n",
       "1     进口食品         海苔片  https://search.jd.com//list.html?cat=1320%2C50...   \n",
       "2     进口食品         水晶米  https://search.jd.com//list.html?cat=1320%2C50...   \n",
       "3     进口食品        果味汽水  https://search.jd.com//list.html?cat=1320%2C50...   \n",
       "4     进口食品         驼奶粉  https://search.jd.com//list.html?cat=1320%2C50...   \n",
       "\n",
       "            n  n clean  \n",
       "0  共1700+ 件商品     1700  \n",
       "1   共300+ 件商品      300  \n",
       "2    共90+ 件商品       90  \n",
       "3  共1300+ 件商品     1300  \n",
       "4   共100+ 件商品      100  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a sample of what the output of this section will look like\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Narrow down category selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JD has A LOT OF products - approximately 2.7 million. I estimate scraping product text data and image data for this many products will take about 90 days (i.e., can probably scrape about 30K products in one day). I also anticipate that scraping this many products will produce about 27 million product images (i.e., 10 images per product), which would equate to about 5400 GB of image data alone! Yikes!\n",
    "\n",
    "I recommend we narrow down the selection of categories from JD. In the 'df' dataset, let's create a column for whether or not we would like to scrape a subcategory. As an example, below I have just scraped information for the subcategory 西柚汁.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a column for whether or not we want to scrape a subcategory\n",
    "\n",
    "# For example, in this code, I have just included 西柚汁\n",
    "df['Include'] = 0\n",
    "df['Include'] = np.where(df['Subcategory'] == '西柚汁', 1, df['Include'])\n",
    "\n",
    "# create list of subcategory URLs of interest\n",
    "subcategory_urls = df[df['Include']==1]['URL'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create spreadsheet of product page URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we access each of subcategory sections. For each advertised product, we collect information on the product name, product price, and product URL. The output from this phase is saved as 'JD_product_URLs.xlsx'\n",
    "\n",
    "The time taken to run this section depends on the number on the number of subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 402 ms, sys: 8.64 ms, total: 411 ms\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = []\n",
    "\n",
    "# iterate through each of the subcategory URLs - can delete '[:5]' below if want to scrape all URLs\n",
    "for subcategory_url in subcategory_urls:\n",
    "    print(subcategory_url)\n",
    "    \n",
    "    # access the subcategory page, this will load 30 products; wait 2 seconds\n",
    "    driver.get(subcategory_url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # scroll down to the bottom of the page to load another 30 products; wait 2 seconds\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # access the HTML from the page\n",
    "    soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    # determine the number of pages for the subcategory\n",
    "    num_pages = int(soup.find('span', {\"class\":\"fp-text\"}).text.split('/')[1])\n",
    "    \n",
    "    # create a list of chunks; one chunk for each product\n",
    "    chunks = soup.find('div', {\"id\":\"J_goodsList\"}).find_all('li')\n",
    "    \n",
    "    # collect desired information\n",
    "    for chunk in chunks:\n",
    "        row = {}\n",
    "        row['Category'] = df[df['URL'] == subcategory_url]['Category'].tolist()[0]\n",
    "        row['Subcategory'] = df[df['URL'] == subcategory_url]['Subcategory'].tolist()[0]\n",
    "        row['Page number'] = 1\n",
    "        row['Product name'] = 1\n",
    "        row['Product price'] = 1\n",
    "        row['Product URL'] = 'https://item.jd.com/' + chunk.get('data-sku') + '.html'\n",
    "        row['Product name'] = chunk.find('a').get('title')\n",
    "        row['Product price'] = chunk.find('div', {\"class\":\"p-price\"}).text.replace('\\n', '').replace('\\n', '')      \n",
    "        data.append(row)\n",
    "        \n",
    "    # repeat the above steps for any additional pages for the category\n",
    "    for num_page in range(3, 2*num_pages+1, 2):\n",
    "        driver.get(subcategory_url + '&page=' + str(num_page))\n",
    "        time.sleep(2)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "        \n",
    "        chunks = soup.find('div', {\"id\":\"J_goodsList\"}).find_all('li')\n",
    "        for chunk in chunks:\n",
    "            row = {}\n",
    "            row['Category'] = df[df['URL'] == subcategory_url]['Category'].tolist()[0]\n",
    "            row['Subcategory'] = df[df['URL'] == subcategory_url]['Subcategory'].tolist()[0]\n",
    "            row['Page number'] = int((num_page+1)/2)\n",
    "            row['Product name'] = 1\n",
    "            row['Product price'] = 1\n",
    "            row['Product URL'] = 'https://item.jd.com/' + chunk.get('data-sku') + '.html'\n",
    "            row['Product name'] = chunk.find('a').get('title')\n",
    "            row['Product price'] = chunk.find('div', {\"class\":\"p-price\"}).text.replace('\\n', '').replace('\\n', '')      \n",
    "            data.append(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Page number</th>\n",
       "      <th>Product name</th>\n",
       "      <th>Product price</th>\n",
       "      <th>Product URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>饮料冲调</td>\n",
       "      <td>西柚汁</td>\n",
       "      <td>1</td>\n",
       "      <td>果满乐乐（gomolo）地中海塞浦路斯进口 100%红心西柚汁 大瓶装纯果汁 1升*4瓶</td>\n",
       "      <td>￥48.00</td>\n",
       "      <td>https://item.jd.com/100021726516.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>饮料冲调</td>\n",
       "      <td>西柚汁</td>\n",
       "      <td>1</td>\n",
       "      <td>上好佳大湖 Great lakes西柚汁果蔬汁饮料1L/瓶婚宴家用大瓶装（新老包装随机发）</td>\n",
       "      <td>￥35.79</td>\n",
       "      <td>https://item.jd.com/10064874457545.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>饮料冲调</td>\n",
       "      <td>西柚汁</td>\n",
       "      <td>1</td>\n",
       "      <td>本小青西柚汁饮料本小青新鲜果汁0脂肪网红解腻饮品300ml*6瓶</td>\n",
       "      <td>￥31.40</td>\n",
       "      <td>https://item.jd.com/100039808885.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>饮料冲调</td>\n",
       "      <td>西柚汁</td>\n",
       "      <td>1</td>\n",
       "      <td>冲调方便，19种口味可选</td>\n",
       "      <td>￥55.00</td>\n",
       "      <td>https://item.jd.com/45538492136.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>饮料冲调</td>\n",
       "      <td>西柚汁</td>\n",
       "      <td>1</td>\n",
       "      <td>云舵 【优选好物】夏季网红饮品本小青西柚汁饮料新鲜维C果汁0脂肪解 西柚汁330ml*12瓶</td>\n",
       "      <td>￥132.30</td>\n",
       "      <td>https://item.jd.com/10066285549681.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category Subcategory  Page number  \\\n",
       "0     饮料冲调         西柚汁            1   \n",
       "1     饮料冲调         西柚汁            1   \n",
       "2     饮料冲调         西柚汁            1   \n",
       "3     饮料冲调         西柚汁            1   \n",
       "4     饮料冲调         西柚汁            1   \n",
       "\n",
       "                                     Product name Product price  \\\n",
       "0    果满乐乐（gomolo）地中海塞浦路斯进口 100%红心西柚汁 大瓶装纯果汁 1升*4瓶        ￥48.00   \n",
       "1   上好佳大湖 Great lakes西柚汁果蔬汁饮料1L/瓶婚宴家用大瓶装（新老包装随机发）        ￥35.79   \n",
       "2               本小青西柚汁饮料本小青新鲜果汁0脂肪网红解腻饮品300ml*6瓶         ￥31.40   \n",
       "3                                    冲调方便，19种口味可选        ￥55.00   \n",
       "4  云舵 【优选好物】夏季网红饮品本小青西柚汁饮料新鲜维C果汁0脂肪解 西柚汁330ml*12瓶       ￥132.30   \n",
       "\n",
       "                               Product URL  \n",
       "0    https://item.jd.com/100021726516.html  \n",
       "1  https://item.jd.com/10064874457545.html  \n",
       "2    https://item.jd.com/100039808885.html  \n",
       "3     https://item.jd.com/45538492136.html  \n",
       "4  https://item.jd.com/10066285549681.html  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(data)\n",
    "\n",
    "# export dataset; the file name can also be changed to '.csv' if preferable\n",
    "df2.to_excel('JD_product_URLs.xlsx', index=False)\n",
    "\n",
    "# see sample of products\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create spreadsheet of product information and product image URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we iterate through each of the product URLs to obtain the product text information and the product image URLs. Importantly, on each page we scroll down to the 商品评价 section so that this section can load and we can scrape the comments images. \n",
    "\n",
    "The time taken to run this section depends on the number on the number of subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "CPU times: user 11.3 s, sys: 142 ms, total: 11.5 s\n",
      "Wall time: 10min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data2 = []\n",
    "\n",
    "# create a counter - this can show us how many product URLs the code has iterated through\n",
    "count = 0\n",
    "\n",
    "# iterate through each of the product URLs\n",
    "product_urls = df2['Product URL'].tolist()\n",
    "for product_url in product_urls:\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        print(count)\n",
    "    \n",
    "    # access product URL; wait 5 seconds to load\n",
    "    driver.get(product_url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # scroll down to the 商品评价 section so that the comments can load; wait 5 seconds\n",
    "    try:\n",
    "        target = driver.find_element_by_id('comment')\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(target).perform()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        None\n",
    "    \n",
    "    # extract HTML from the product page\n",
    "    soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    # create a dictionary for the given product\n",
    "    row = {}\n",
    "    \n",
    "    # create a field for the product URL\n",
    "    row['001_Product URL'] = product_url\n",
    "     \n",
    "    # extract the category information\n",
    "    try:\n",
    "        row['002_Category level 1'] = soup.find('div', {\"class\":\"crumb fl clearfix\"}).find('a', {\"clstag\":\"shangpin|keycount|product|mbNav-2\"}).text\n",
    "    except:\n",
    "        None\n",
    "    try:\n",
    "        row['003_Category level 2'] = soup.find('div', {\"class\":\"crumb fl clearfix\"}).find('a', {\"clstag\":\"shangpin|keycount|product|mbNav-3\"}).text\n",
    "    except:\n",
    "        None\n",
    "    try:\n",
    "        row['004_Category level 3'] = soup.find('div', {\"class\":\"crumb fl clearfix\"}).find('a', {\"clstag\":\"shangpin|keycount|product|mbNav-4\"}).text\n",
    "    except:\n",
    "        None\n",
    "    \n",
    "    # extract the brand name\n",
    "    try:\n",
    "        row['005_Brand name'] = soup.find('div', {\"class\":\"crumb fl clearfix\"}).find('a', {\"clstag\":\"shangpin|keycount|product|mbNav-5\"}).text\n",
    "    except:\n",
    "        None\n",
    "    \n",
    "    # extract the product name\n",
    "    try:\n",
    "        row['006_Product name'] = soup.find('div', {\"class\":\"sku-name\"}).text.replace('\\n', '').lstrip().rstrip()\n",
    "    except:\n",
    "        None\n",
    "    \n",
    "    # extract the product price\n",
    "    try:\n",
    "        row['007_Price'] = soup.find('div', {\"class\":\"summary-price-wrap\"}).find('span', {\"class\":\"p-price\"}).text.replace('\\n', '')\n",
    "    except:\n",
    "        None\n",
    "    \n",
    "    # extract the URLs for the images in the left panel (first 5 images)\n",
    "    try:   \n",
    "        product_photos = ['https:' + x.get('src').replace('n5','n0').replace('.avif', '').replace('https:https:', 'https:') for x in soup.find('div', {\"id\":\"spec-list\"}).find_all('img')]\n",
    "        num_product_photos = len(product_photos)\n",
    "        for n in range(0, num_product_photos):\n",
    "            if (n < 5):\n",
    "                row['008_Product photo ' + str(n)] = product_photos[n].replace('.gif', '')\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    # extract the URLs for the images in the 商品介绍 section (first 5 images)\n",
    "    try:\n",
    "        #description_photos = ['https:' + x.get('src').replace('.avif', '').replace('https:https:', 'https:') for x in soup.find('div', {\"id\":\"J-detail-content\"}).find_all('img')]\n",
    "        description_photos = [x.split('.avif')[0].replace(\"(\", 'https:') for x in str(soup.find('div', {\"id\":\"J-detail-content\"})).split('background-image:url')[1:-1]]\n",
    "        \n",
    "        num_description_photos = len(description_photos)\n",
    "        for n in range(0, num_description_photos):\n",
    "            if (n < 5):\n",
    "                row['009_Description photo ' + str(n)] = description_photos[n].replace('.gif', '')\n",
    "    except:\n",
    "        None\n",
    "  \n",
    "    # extract the URLs for the images in the 商品评价 section (first 10)\n",
    "    try:\n",
    "        comment_photos = ['https:' + x.get('src').replace('n0/s48x48_jfs/', 'shaidan/s616x405_jfs/').replace('.avif', '').replace('https:https:', '') for x in soup.find('div', {\"id\":\"comment-0\"}).find_all('img')]\n",
    "        comment_photos = [x for x in comment_photos if 'https://img' == x[:11]]\n",
    "        num_comment_photos = len(comment_photos)\n",
    "        for n in range(0, num_comment_photos):\n",
    "            if (n < 10):\n",
    "                row['010_Comment photo ' + str(n)] = comment_photos[n].replace('.gif', '')\n",
    "    except:\n",
    "        None\n",
    "    \n",
    "    # append the product information to 'data2'\n",
    "    data2.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product URL</th>\n",
       "      <th>Category level 1</th>\n",
       "      <th>Category level 2</th>\n",
       "      <th>Category level 3</th>\n",
       "      <th>Brand name</th>\n",
       "      <th>Product name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Product photo 0</th>\n",
       "      <th>Product photo 1</th>\n",
       "      <th>Product photo 2</th>\n",
       "      <th>Product photo 3</th>\n",
       "      <th>Product photo 4</th>\n",
       "      <th>Description photo 0</th>\n",
       "      <th>Description photo 1</th>\n",
       "      <th>Description photo 2</th>\n",
       "      <th>Description photo 3</th>\n",
       "      <th>Description photo 4</th>\n",
       "      <th>Comment photo 0</th>\n",
       "      <th>Comment photo 1</th>\n",
       "      <th>Comment photo 2</th>\n",
       "      <th>SKU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://item.jd.com/100021726516.html</td>\n",
       "      <td>饮料冲调</td>\n",
       "      <td>饮料</td>\n",
       "      <td>果蔬汁/饮料</td>\n",
       "      <td>果满乐乐（gomolo）</td>\n",
       "      <td>果满乐乐（gomolo）地中海塞浦路斯进口 100%红心西柚汁 大瓶装纯果汁 1升*4瓶</td>\n",
       "      <td>￥48.00</td>\n",
       "      <td>https://img11.360buyimg.com/n0/jfs/t1/98350/21...</td>\n",
       "      <td>https://img11.360buyimg.com/n0/jfs/t1/119583/4...</td>\n",
       "      <td>https://img11.360buyimg.com/n0/jfs/t1/34617/22...</td>\n",
       "      <td>https://img11.360buyimg.com/n0/jfs/t1/158775/3...</td>\n",
       "      <td>https://img11.360buyimg.com/n0/jfs/t1/42644/24...</td>\n",
       "      <td>https://img30.360buyimg.com/sku/jfs/t1/125420/...</td>\n",
       "      <td>https://img30.360buyimg.com/sku/jfs/t1/77393/3...</td>\n",
       "      <td>https://img30.360buyimg.com/sku/jfs/t1/78564/3...</td>\n",
       "      <td>https://img30.360buyimg.com/sku/jfs/t1/83012/3...</td>\n",
       "      <td>https://img30.360buyimg.com/sku/jfs/t1/184610/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100021726516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://item.jd.com/10064874457545.html</td>\n",
       "      <td>饮料冲调</td>\n",
       "      <td>饮料</td>\n",
       "      <td>果蔬汁/饮料</td>\n",
       "      <td>雨小姐（YUXIAOJIE）</td>\n",
       "      <td>上好佳大湖 Great lakes西柚汁果蔬汁饮料1L/瓶婚宴家用大瓶装（新老包装随机发）</td>\n",
       "      <td>￥35.79</td>\n",
       "      <td>https://img10.360buyimg.com/n0/jfs/t1/29201/1/...</td>\n",
       "      <td>https://img10.360buyimg.com/n0/jfs/t1/156514/2...</td>\n",
       "      <td>https://img10.360buyimg.com/n0/jfs/t1/134967/1...</td>\n",
       "      <td>https://img10.360buyimg.com/n0/jfs/t1/169016/5...</td>\n",
       "      <td>https://img10.360buyimg.com/n0/jfs/t1/90430/3/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10064874457545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Product URL Category level 1 Category level 2  \\\n",
       "0    https://item.jd.com/100021726516.html             饮料冲调               饮料   \n",
       "1  https://item.jd.com/10064874457545.html             饮料冲调               饮料   \n",
       "\n",
       "  Category level 3      Brand name  \\\n",
       "0           果蔬汁/饮料    果满乐乐（gomolo）   \n",
       "1           果蔬汁/饮料  雨小姐（YUXIAOJIE）   \n",
       "\n",
       "                                    Product name   Price  \\\n",
       "0   果满乐乐（gomolo）地中海塞浦路斯进口 100%红心西柚汁 大瓶装纯果汁 1升*4瓶  ￥48.00   \n",
       "1  上好佳大湖 Great lakes西柚汁果蔬汁饮料1L/瓶婚宴家用大瓶装（新老包装随机发）  ￥35.79   \n",
       "\n",
       "                                     Product photo 0  \\\n",
       "0  https://img11.360buyimg.com/n0/jfs/t1/98350/21...   \n",
       "1  https://img10.360buyimg.com/n0/jfs/t1/29201/1/...   \n",
       "\n",
       "                                     Product photo 1  \\\n",
       "0  https://img11.360buyimg.com/n0/jfs/t1/119583/4...   \n",
       "1  https://img10.360buyimg.com/n0/jfs/t1/156514/2...   \n",
       "\n",
       "                                     Product photo 2  \\\n",
       "0  https://img11.360buyimg.com/n0/jfs/t1/34617/22...   \n",
       "1  https://img10.360buyimg.com/n0/jfs/t1/134967/1...   \n",
       "\n",
       "                                     Product photo 3  \\\n",
       "0  https://img11.360buyimg.com/n0/jfs/t1/158775/3...   \n",
       "1  https://img10.360buyimg.com/n0/jfs/t1/169016/5...   \n",
       "\n",
       "                                     Product photo 4  \\\n",
       "0  https://img11.360buyimg.com/n0/jfs/t1/42644/24...   \n",
       "1  https://img10.360buyimg.com/n0/jfs/t1/90430/3/...   \n",
       "\n",
       "                                 Description photo 0  \\\n",
       "0  https://img30.360buyimg.com/sku/jfs/t1/125420/...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                 Description photo 1  \\\n",
       "0  https://img30.360buyimg.com/sku/jfs/t1/77393/3...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                 Description photo 2  \\\n",
       "0  https://img30.360buyimg.com/sku/jfs/t1/78564/3...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                 Description photo 3  \\\n",
       "0  https://img30.360buyimg.com/sku/jfs/t1/83012/3...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                 Description photo 4 Comment photo 0  \\\n",
       "0  https://img30.360buyimg.com/sku/jfs/t1/184610/...             NaN   \n",
       "1                                                NaN             NaN   \n",
       "\n",
       "  Comment photo 1 Comment photo 2             SKU  \n",
       "0             NaN             NaN    100021726516  \n",
       "1             NaN             NaN  10064874457545  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe of product information\n",
    "df3 = pd.DataFrame(data2)\n",
    "\n",
    "# change column names\n",
    "cols = df3.columns.tolist()\n",
    "cols.sort()\n",
    "df3 = df3[cols]\n",
    "df3.columns = [x[4:] for x in df3.columns.tolist()]\n",
    "\n",
    "# limit product photos\n",
    "\n",
    "# create a variable for the SKU\n",
    "df3['SKU'] = df3['Product URL'].str.split('/').str[-1].str.split('.html').str[0].astype(int)\n",
    "\n",
    "# export dataset\n",
    "df3.to_excel('JD_product_info.xlsx', index=False)\n",
    "\n",
    "# see sample of products\n",
    "df3.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I note the section above does consistently scrape products from the product section, but inconsistently from the 商品介绍 section and the 商品评价 section - I'm not sure why this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Scrape product images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we access the product images URLs and download them. \n",
    "\n",
    "The time taken to run this section depends on the number on the number of subcategories. The 西柚汁 alone produced 629 images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image URL scraper\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-Agent', 'MyApp/1.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "CPU times: user 6.02 s, sys: 1.06 s, total: 7.07 s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# prepare list of product SKUs to scrape\n",
    "SKU_list = df3['SKU'].tolist()\n",
    "\n",
    "# use counter\n",
    "count = 0\n",
    "error_count = 0\n",
    "\n",
    "# scrape the product images for each SKU, for example's sake I capped at 20\n",
    "for SKU in SKU_list:\n",
    "    count += 1\n",
    "    print(count)\n",
    "    \n",
    "    # determine list of image URLs from spreadsheet\n",
    "    image_urls = df3[df3['SKU']==SKU].iloc[0][7:-1].dropna().tolist()\n",
    "    num_image_urls = len(image_urls)\n",
    "    \n",
    "    # scrape image URLs\n",
    "    for num in range(0, num_image_urls):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(image_urls[num], str(SKU) + '|' + str(num+10) + \".jpg\") \n",
    "        except:\n",
    "            error_count += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
